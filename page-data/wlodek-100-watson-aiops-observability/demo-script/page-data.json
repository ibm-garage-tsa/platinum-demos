{"componentChunkName":"component---src-pages-wlodek-100-watson-aiops-observability-demo-script-mdx","path":"/wlodek-100-watson-aiops-observability/demo-script/","result":{"pageContext":{"frontmatter":{"title":"Observability 100-level live demo","description":"Observability 100-level live demo","tabs":["Demo preparation","Demo script"]},"relativePagePath":"/wlodek-100-watson-aiops-observability/demo-script.mdx","titleType":"page","MdxNode":{"id":"ef565273-51d5-580a-b5da-e2dc431a073d","children":[],"parent":"0c34731a-e301-5176-a2c0-9fab2708041a","internal":{"content":"---\ntitle: Observability 100-level live demo\ndescription: Observability 100-level live demo\ntabs: [ 'Demo preparation', 'Demo script']\n---\n\nexport const Title = () => (\n  <span>\n    Observability <br /> 100-level live demo\n  </span>\n);\n\n![banner](./images/AIOps_Observability_100_Script.jpg)\n\n<span id=\"top\"></span>\n\n<details>\n\n<summary>Introduction</summary>\n<br/>\n\n| **Details** |   |\n| :---                   | :--- |\n| **Actions**            | From sidebar menu, click  **Applications**. Choose **Robot Shop.** |\n| **Narration**          | In this demo, I’ll show how IBM Instana helps quickly identify, debug, and resolve an incident in a microservices-based application. <br/> <br/> To set the context, our application is called Stan’s Robot Shop, a modern, cloud-native application with microservices leveraging various technologies such as Java, Python, and MySQL and deployed in containers on top of a Kubernetes cluster.<br/> <br/> Such applications create a serious challenge for managing application performance, because components are dynamic and loosly coupled. These applications use different technologies, usually requiring broad knowledge and multiple tools to diagnose.<br/> <br/>Instana, with a single agent deployed per host, automates the discovery process. Application components are discovered and observed as they are deployed. Over 200 technologies are supported with zero or minimal configuration, releasing you from installing and configuring multiple tools or plugins. The discovered components can be grouped into an Application perspective, giving the application owner an overview of key metrics (\"golden signals\") like *Traffic*, *Errors* and *Latency* on a single pane of glass. |\n| **Screenshot**         | <br/> ![robot_shop_starting_point](./images/1_introduction_1.png) |\n\n\n(Printer-ready PDF of demo script <a href=\"./100-Observability-Demo-Script.pdf\" target=\"_blank\" rel=\"noreferrer\">here</a>)\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n<details>\n\n<summary>1 - Context</summary>\n<br/>\n\n| **1.1** |  **Automated discovery and dependency mapping** |\n| :---                   | :--- |\n| **Actions**            | Click the **Dependencies** tab. |\n| **Narration**          | Context is the interrelated conditions in which something exists or occurs; context puts things into perspective. Understanding the context of the observability data is essential with modern distributed applications where there are extensive dependencies between components. What you can see here is a visualization of all the dependencies within the Robot Shop application. Instana automatically discovered the relationships between the services and correlated them into this dynamic graph. This provides necessery context to understand how different components affect each other. We can see how requests are moving through the application in real-time. Instana is able to do this because it tracks every request that flows through the application. <br/> <br/> We can tell there are some problems with the application because several of the services are highlighted in yellow and red. <br/> <br/> But, you wouldn’t normally be looking at the dependency map when something like this happens, so let me walk you through what it looks like from the Site Reliability Engineer (SRE) or IT Operator’s point of view when an incident occurs. |\n| **Screenshot**         | <br/> ![robot_shop_dashboard](./images/2_discovery_1.png)<br/>![robot_shop_dependency_map](./images/2_discovery_2.png) |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>2 - Working with events and incidents</summary>\n<br/>\n\n| **2.1** | **Automatically assessing events and alerts** |\n| :---                   | :--- |\n| **Action**             | Click on the **Events** icon (triangle) in sidebar menu. |\n| **Narration**          | We’ve just gotten an alert from Instana that there has been a sudden increase in erroneous calls on our ‘discount’ service, which is part of the robot shop application. <br/> <br/> Although I don’t have it connected right now, the alert would show up via one of the configurable alert channels, like Pager Duty, Microsoft Teams, Slack, and many others (<a href=\"https://www.instana.com/docs/events_alerts/alert-channels\" target=\"_blank\" rel=\"noreferrer\">full list</a>). <br/> <br/> It’s important to note here that you’re not getting alerts for just anything. Behind the scenes, Instana is determining which events and issues are related, and it only sends alerts on incidents if a problem is likely to affect end users. <br/> <br/> Let’s go into the details for this incident. |\n| **Screenshot**         | <br/> ![sidebar_menu](./images/3_events_1.png) |\n<br/>\n\n| **2.2** | **Inspecting auto-correlated incident details** |\n| :---                   | :--- |\n| **Action**             | Click on the incident called **Sudden increase in the number of erroneous calls** on the 'discount' service. |\n| **Narration**          | Instana recognized that the sudden increase in the number of erroneous calls was something important to alert on, so we did not have to do any configuration or set thresholds in order to get this alert. We get key information right away when we come into this incident detail page. There’s a timeline of the incident, the event that triggered Instana to create the incident, and all of the related events. |\n| **Screenshots**        | <br/> ![event_page](./images/3_events_2.png) <br/> <br/> Incident details screen: <br/> ![incident_details_screen](./images/3_events_3.png) |\n\n<br/>\n\n| **2.3** | **Understand the incident** |\n| :---                   | :--- |\n| **Actions**            | Under **Related Events**, click on the event that says **Sudden increase in the number of erroneous calls**. At the end, click **Analyze Calls** button. |\n| **Narration**          | It looks like the abnormal termination of the MySQL database caused the problem. It shows how one data store issue rippled out to effect a number of directly and indirectly connected services. Instana’s automatic root cause analysis uses the relationship information from the Dynamic Graph to accurately collate the individual issues into one incident. This completely eliminates alert storms. Providing your DevOps engineers and SREs with a single notification of actionable information to enable them to promptly restore normal service. Let’s look at some related traces for this. |\n| **Screenshot**         | <br/> ![events](./images/3_events_4.png) |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>3 - Unbounded Analytics </summary>\n<br/>\n\n| **3.1** | **Examine call details** |\n| :---                   | :--- |\n| **Actions**            | Click on the endpoint named **CONNECT**. Then, click on the first call (also named **CONNECT**).|\n| **Narration**          | Now, we moved to the Analytics view. You can see, how Instana UI allows for easy navigation between different views, keeping the time span and context. At the top you can see Filter that was applied to all collected traces. All filtered requests are grouped by endpoint [*In this case it is database CONNECT exposed my MySQL server*]. There is only one endpoint here, but if there were multiple, you’d see a list. Endpoints are automatically discovered and mapped by Instana. <br/><br/> We can go into the details for each erroneous call to MySQL via this endpoint (CONNECT). |\n| **Screenshot**         | <br/> ![endpoint_connect](./images/4_traces_1.png) |\n\n<br/>\n\n| **3.2** | **View call via visual dashboard** |\n| :---                   | :--- |\n| **Action**             | -- |\n| **Narration**          | Clicking on an individual call takes us to a view of the call in context of the end-to-end trace. We can see where the request began () and each call that was made along the way. <br/> <br/> Everything is presented in an easy-to-navigate visual dashboard, so we can drill into increasingly detailed information to pinpoint the problem, without using multiple tools or navigating back and forth to lots of dashboards. |\n| **Screenshot**         | <br/> ![call_timeline](./images/4_traces_2.png) |\n\n<br/>\n\n| **3.3** | **Understand the impact and source of the incident** |\n| :---                   | :--- |\n| **Action**             | Click on the span called **CONNECT** and refer to the sidebar on the right side. |\n| **Narration**          | Clicking on a span gives more details, including the source and destination as well as the full stack trace. In this case, we can see that the source is the ‘discount’ service, and [scroll down] the destination is CONNECT of MySQL. <br/> <br/> So we can confirm that the root cause of the incident that affected the ‘discount’ service was with the MySQL database. The abnormal termination of the database caused a connection error, which then flowed back through the application. <br/> <br/> When we bring MySQL back online, it will fix the problem. |\n| **Screenshot**         | <br/> ![connect](./images/4_traces_3.png) |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>4 - Confirming incident resolution was successful</summary>\n<br/>\n\n| **4.1** | **Metrics for the Robot Shop have returned to normal** |\n| :---                   | :--- |\n| **Actions**            | In the top-right corner set the timeframe so it begins at 30 minutes past the hour and ends at 45 minutes past the hour. Click **Set time** <br/> <br/> Navigate to **Applications** in the sidebar menu, choose **Robot Shop**, and click on the **Summary** tab. <br/> <br/> Note: You should see that the call volume has increased, the number of erroneous calls decreased, and latency also decreased.  |\n| **Narration**          | Now that MySQL is working again, we can go back and confirm that the problems with the Robot Shop have been repaired. |\n| **Screenshot**         | <br/> ![robot_shop_summary_end](./images/5_confirmation_1.png) |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>Summary</summary>\n<br/>\n\nNow, we can see that the metrics for the Robot Shop have returned to normal: the call volume has increased again, the erroneous call rate as well as latency has dropped.\n\n<br/>\n\nThe problem with the Robot Shop has been fixed and the application normal state was restored!\n\n<br/>\n\nHopefully, you’ve seen that Instana can help make the process of identifying problems and finding the root cause of those problems very frictionless. Since Instana automates so many of the manual and labor-intensive aspects of the process, you can focus on getting other work done and not worry about instrumenting observability or constantly monitoring for problems. And when problems do arise, all the trace data is there at your fingertips to dig into.\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n***\n","type":"Mdx","contentDigest":"2c66894a3f43877e536682ffd60335f7","owner":"gatsby-plugin-mdx","counter":2411},"frontmatter":{"title":"Observability 100-level live demo","description":"Observability 100-level live demo","tabs":["Demo preparation","Demo script"]},"exports":{},"rawBody":"---\ntitle: Observability 100-level live demo\ndescription: Observability 100-level live demo\ntabs: [ 'Demo preparation', 'Demo script']\n---\n\nexport const Title = () => (\n  <span>\n    Observability <br /> 100-level live demo\n  </span>\n);\n\n![banner](./images/AIOps_Observability_100_Script.jpg)\n\n<span id=\"top\"></span>\n\n<details>\n\n<summary>Introduction</summary>\n<br/>\n\n| **Details** |   |\n| :---                   | :--- |\n| **Actions**            | From sidebar menu, click  **Applications**. Choose **Robot Shop.** |\n| **Narration**          | In this demo, I’ll show how IBM Instana helps quickly identify, debug, and resolve an incident in a microservices-based application. <br/> <br/> To set the context, our application is called Stan’s Robot Shop, a modern, cloud-native application with microservices leveraging various technologies such as Java, Python, and MySQL and deployed in containers on top of a Kubernetes cluster.<br/> <br/> Such applications create a serious challenge for managing application performance, because components are dynamic and loosly coupled. These applications use different technologies, usually requiring broad knowledge and multiple tools to diagnose.<br/> <br/>Instana, with a single agent deployed per host, automates the discovery process. Application components are discovered and observed as they are deployed. Over 200 technologies are supported with zero or minimal configuration, releasing you from installing and configuring multiple tools or plugins. The discovered components can be grouped into an Application perspective, giving the application owner an overview of key metrics (\"golden signals\") like *Traffic*, *Errors* and *Latency* on a single pane of glass. |\n| **Screenshot**         | <br/> ![robot_shop_starting_point](./images/1_introduction_1.png) |\n\n\n(Printer-ready PDF of demo script <a href=\"./100-Observability-Demo-Script.pdf\" target=\"_blank\" rel=\"noreferrer\">here</a>)\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n<details>\n\n<summary>1 - Context</summary>\n<br/>\n\n| **1.1** |  **Automated discovery and dependency mapping** |\n| :---                   | :--- |\n| **Actions**            | Click the **Dependencies** tab. |\n| **Narration**          | Context is the interrelated conditions in which something exists or occurs; context puts things into perspective. Understanding the context of the observability data is essential with modern distributed applications where there are extensive dependencies between components. What you can see here is a visualization of all the dependencies within the Robot Shop application. Instana automatically discovered the relationships between the services and correlated them into this dynamic graph. This provides necessery context to understand how different components affect each other. We can see how requests are moving through the application in real-time. Instana is able to do this because it tracks every request that flows through the application. <br/> <br/> We can tell there are some problems with the application because several of the services are highlighted in yellow and red. <br/> <br/> But, you wouldn’t normally be looking at the dependency map when something like this happens, so let me walk you through what it looks like from the Site Reliability Engineer (SRE) or IT Operator’s point of view when an incident occurs. |\n| **Screenshot**         | <br/> ![robot_shop_dashboard](./images/2_discovery_1.png)<br/>![robot_shop_dependency_map](./images/2_discovery_2.png) |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>2 - Working with events and incidents</summary>\n<br/>\n\n| **2.1** | **Automatically assessing events and alerts** |\n| :---                   | :--- |\n| **Action**             | Click on the **Events** icon (triangle) in sidebar menu. |\n| **Narration**          | We’ve just gotten an alert from Instana that there has been a sudden increase in erroneous calls on our ‘discount’ service, which is part of the robot shop application. <br/> <br/> Although I don’t have it connected right now, the alert would show up via one of the configurable alert channels, like Pager Duty, Microsoft Teams, Slack, and many others (<a href=\"https://www.instana.com/docs/events_alerts/alert-channels\" target=\"_blank\" rel=\"noreferrer\">full list</a>). <br/> <br/> It’s important to note here that you’re not getting alerts for just anything. Behind the scenes, Instana is determining which events and issues are related, and it only sends alerts on incidents if a problem is likely to affect end users. <br/> <br/> Let’s go into the details for this incident. |\n| **Screenshot**         | <br/> ![sidebar_menu](./images/3_events_1.png) |\n<br/>\n\n| **2.2** | **Inspecting auto-correlated incident details** |\n| :---                   | :--- |\n| **Action**             | Click on the incident called **Sudden increase in the number of erroneous calls** on the 'discount' service. |\n| **Narration**          | Instana recognized that the sudden increase in the number of erroneous calls was something important to alert on, so we did not have to do any configuration or set thresholds in order to get this alert. We get key information right away when we come into this incident detail page. There’s a timeline of the incident, the event that triggered Instana to create the incident, and all of the related events. |\n| **Screenshots**        | <br/> ![event_page](./images/3_events_2.png) <br/> <br/> Incident details screen: <br/> ![incident_details_screen](./images/3_events_3.png) |\n\n<br/>\n\n| **2.3** | **Understand the incident** |\n| :---                   | :--- |\n| **Actions**            | Under **Related Events**, click on the event that says **Sudden increase in the number of erroneous calls**. At the end, click **Analyze Calls** button. |\n| **Narration**          | It looks like the abnormal termination of the MySQL database caused the problem. It shows how one data store issue rippled out to effect a number of directly and indirectly connected services. Instana’s automatic root cause analysis uses the relationship information from the Dynamic Graph to accurately collate the individual issues into one incident. This completely eliminates alert storms. Providing your DevOps engineers and SREs with a single notification of actionable information to enable them to promptly restore normal service. Let’s look at some related traces for this. |\n| **Screenshot**         | <br/> ![events](./images/3_events_4.png) |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>3 - Unbounded Analytics </summary>\n<br/>\n\n| **3.1** | **Examine call details** |\n| :---                   | :--- |\n| **Actions**            | Click on the endpoint named **CONNECT**. Then, click on the first call (also named **CONNECT**).|\n| **Narration**          | Now, we moved to the Analytics view. You can see, how Instana UI allows for easy navigation between different views, keeping the time span and context. At the top you can see Filter that was applied to all collected traces. All filtered requests are grouped by endpoint [*In this case it is database CONNECT exposed my MySQL server*]. There is only one endpoint here, but if there were multiple, you’d see a list. Endpoints are automatically discovered and mapped by Instana. <br/><br/> We can go into the details for each erroneous call to MySQL via this endpoint (CONNECT). |\n| **Screenshot**         | <br/> ![endpoint_connect](./images/4_traces_1.png) |\n\n<br/>\n\n| **3.2** | **View call via visual dashboard** |\n| :---                   | :--- |\n| **Action**             | -- |\n| **Narration**          | Clicking on an individual call takes us to a view of the call in context of the end-to-end trace. We can see where the request began () and each call that was made along the way. <br/> <br/> Everything is presented in an easy-to-navigate visual dashboard, so we can drill into increasingly detailed information to pinpoint the problem, without using multiple tools or navigating back and forth to lots of dashboards. |\n| **Screenshot**         | <br/> ![call_timeline](./images/4_traces_2.png) |\n\n<br/>\n\n| **3.3** | **Understand the impact and source of the incident** |\n| :---                   | :--- |\n| **Action**             | Click on the span called **CONNECT** and refer to the sidebar on the right side. |\n| **Narration**          | Clicking on a span gives more details, including the source and destination as well as the full stack trace. In this case, we can see that the source is the ‘discount’ service, and [scroll down] the destination is CONNECT of MySQL. <br/> <br/> So we can confirm that the root cause of the incident that affected the ‘discount’ service was with the MySQL database. The abnormal termination of the database caused a connection error, which then flowed back through the application. <br/> <br/> When we bring MySQL back online, it will fix the problem. |\n| **Screenshot**         | <br/> ![connect](./images/4_traces_3.png) |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>4 - Confirming incident resolution was successful</summary>\n<br/>\n\n| **4.1** | **Metrics for the Robot Shop have returned to normal** |\n| :---                   | :--- |\n| **Actions**            | In the top-right corner set the timeframe so it begins at 30 minutes past the hour and ends at 45 minutes past the hour. Click **Set time** <br/> <br/> Navigate to **Applications** in the sidebar menu, choose **Robot Shop**, and click on the **Summary** tab. <br/> <br/> Note: You should see that the call volume has increased, the number of erroneous calls decreased, and latency also decreased.  |\n| **Narration**          | Now that MySQL is working again, we can go back and confirm that the problems with the Robot Shop have been repaired. |\n| **Screenshot**         | <br/> ![robot_shop_summary_end](./images/5_confirmation_1.png) |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>Summary</summary>\n<br/>\n\nNow, we can see that the metrics for the Robot Shop have returned to normal: the call volume has increased again, the erroneous call rate as well as latency has dropped.\n\n<br/>\n\nThe problem with the Robot Shop has been fixed and the application normal state was restored!\n\n<br/>\n\nHopefully, you’ve seen that Instana can help make the process of identifying problems and finding the root cause of those problems very frictionless. Since Instana automates so many of the manual and labor-intensive aspects of the process, you can focus on getting other work done and not worry about instrumenting observability or constantly monitoring for problems. And when problems do arise, all the trace data is there at your fingertips to dig into.\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n***\n","fileAbsolutePath":"/home/runner/work/platinum-demos/platinum-demos/src/pages/wlodek-100-watson-aiops-observability/demo-script.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}