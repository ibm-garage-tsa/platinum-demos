{"componentChunkName":"component---src-pages-300-watson-aiops-observability-demo-script-mdx","path":"/300-watson-aiops-observability/demo-script/","result":{"pageContext":{"frontmatter":{"title":"Observability 300-level live demo","description":"Observability 300-level live demo","tabs":["Demo preparation","Demo script"]},"relativePagePath":"/300-watson-aiops-observability/demo-script.mdx","titleType":"page","MdxNode":{"id":"728c2dee-e9f2-55ce-9db7-238b0a8fdfd3","children":[],"parent":"f7966555-1124-5a05-857e-0e39208db440","internal":{"content":"---\ntitle: Observability\n  300-level live demo\ndescription: Observability 300-level live demo\ntabs: [ 'Demo preparation', 'Demo script']\n---\n\nexport const Title = () => (\n  <span>\n    Observability <br/> 300-level live demo\n  </span> );\n\n![banner](./images/AIOps_Observability_300_Script.jpg)\n\n<span id=\"top\"></span>\n\n<details>\n\n<summary>Introduction</summary>\n<br/>\n\n| **Starting point** | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| :--- | :--- |\n| **Actions** | From sidebar menu, click on **Applications**<br/><br/>Choose **Robot Shop** <br/><br/>Click on the **Dependencies** tab |\n| **Narration** | In this demo, I'll show how IBM Instana helps quickly identify, debug, and resolve an incident in a microservices-based application. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Screenshot** | <br/> ![robot_shop_starting_point](./images/robot_shop_starting_point_600.png) |\n\n<br/>\n\n| **Context of demo:** | **Robot shop application** |\n| :--- | :--- |\n| **Action** | Hover over a few of the icons to show info on what technology they are built on |\n| **Narration** | To set the context, our application is called Stan's Robot Shop, and it uses various technologies such as Java, Python, and MySQL.<br/><br/>This is a visualization of all the dependencies within the robot shop application. Instana automatically discovered the relationships between the services and correlated them into this dynamic graph. We can see how requests are moving through the application in real time. Instana is able to do this because it tracks every request that flows through the application.<br/><br/>We can tell there are some problems with the application because several of the services are highlighted in yellow and red.<br/><br/>But, you wouldn't normally be looking at the dashboard when something like this happens, so let me walk you through what it looks like from the SRE/IT operator's point of view when an incident occurs. |\n| **Screenshot** | <br/> ![robot_shop_starting_point](./images/robot_shop_starting_point_600.png) |\n\n<br/>\n\n(Printer-ready PDF of demo script <a href=\"./300-Observability-Demo-Script.pdf\" target=\"_blank\" rel=\"noreferrer\">here</a>)\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>1 - Getting an incident alert</summary>\n<br/>\n\n| **1.1** | **Automatically assessing events and alerts** |\n| :--- | :--- |\n| **Action** | Click on the **Events** icon (triangle) in sidebar menu &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Narration** | We've just gotten an alert from Instana that there has been a sudden increase in erroneous calls on our 'discount' service, which is part of the robot shop application. <br/> <br/> Although I don't have it connected right now, the alert would show up via one of the configurable alert channels, like email, Microsoft Teams, Slack, and many others (<a href=\"https://www.instana.com/docs/events_alerts/alert-channels\" target=\"_blank\" rel=\"noreferrer\">full list</a>). <br/><br/>It's important to note here that you're not getting alerts for just anything. Instana automatically groups related events and issues into incidents. It determines that events and/or issues are related using the dynamic dependency graph that we just looked at. And Instana continuously assesses the groups of events and issues to determine which ones are impacting end users or posing an imminent risk of impacting end users. Those are the ones that Instana will alert on, so as a SRE/IT operator, you will not be interrupted constantly for things that are not very important. <br/><br/> Let's go into the details for this incident. |\n| **Screenshot** | <br/> ![sidebar_menu](./images/sidebar_menu_600.png) |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>2 - Inspect auto-correlated incident details</summary>\n<br/>\n\n| **2.1** | **Gather information from incident detail page** |\n| :--- | :--- |\n| **Action** | Click on the incident called **Sudden increase in the number of erroneous calls** on the 'discount' service &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Narration** | Instana recognized that the sudden increase in the number of erroneous calls was something important to alert on, so we did not have to do any configuration or set thresholds in order to get this alert. We get key information right away when we come into this incident detail page. There's a timeline of the incident, the event that triggered Instana to create the incident, and all of the related events. |\n| **Screenshots** | <br/> ![event_page](./images/event_page_600.png) <br/> <br/> <br/> ![incident_details_screen](./images/incident_details_screen_600.png) |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>3 - Debugging the incident by inspecting calls</summary>\n<br/>\n\n| **3.1** | ** Understand the incident**  |\n| :--- | :--- |\n| **Actions** | Click on the event that says **Sudden increase in the number of erroneous calls**<br/><br/>Then, click **Analyze Calls** &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Narration** | By inspecting the related events, it looks like the abnormal termination of the MySQL database caused the problem. We can go into more detail about each call that failed to connect to the database, by choosing the event that relates to the sudden increase in erroneous calls, and going to analyze calls. Going into the actual trace for a request that resulted in an error will help us confirm that MySQL is really the source of the incident.|\n| **Screenshot** | <br/> ![events](./images/events_600.png) |\n\n<br/>\n\n| **3.2** | **Examine details** |\n| :--- | :--- |\n| **Actions** | Click on the endpoint named **CONNECT**<br/><br/> Then, click on the first call (also named **CONNECT**) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Narration** | All the calls are grouped by endpoint. There is only one endpoint here, but if there were multiple, you'd see a list here. Endpoints are automatically discovered and mapped by Instana. <br/> <br/> We can go into the details for each erroneous call to MySQL via this endpoint (CONNECT). |\n| **Screenshot** | <br/> ![endpoint_connect](./images/endpoint_connect_600.png) |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>4 - Drill down with end-to-end traces</summary>\n<br/>\n\n| **4.1** | **View call via visual dashboard** |\n| :--- | :--- |\n| **Action** | -- |\n| **Narration** | Let's take a look at the first call in the list. Clicking on an individual call takes us to a view of the call in context of the end-to-end trace. We can see where the request began and each call that was made along the way. The timeline view gives a quick overview of the time spent on each span, as well as key performance indicators, such as the number of erroneous calls in this trace, the number of warning logs, and total latency.<br/><br/>Everything is presented in an easy-to-navigate visual dashboard, so we can drill into increasingly detailed information to pinpoint the problem, without using multiple tools or navigating back and forth to lots of dashboards. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Screenshot** | <br/> ![call_timeline](./images/call_timeline_600.png) |\n\n<br/>\n\n| **4.2** | **Understand the impact and source of the incident** |\n| :--- | :--- |\n| **Action** | Scroll down to the section labeled **Calls** &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Narration** | In the call stack, we can click on each span to see more information, including the complete stack trace. We can see the source, in this case the 'discount' service, and [scroll down] the destination, which in this case is CONNECT of MySQL. <br/> <br/> It's useful to have this context because we can easily see how the calls go from one service to another, just by clicking on them. We can also see how the error (red triangle) propagated up the call stack, in this case beginning with the MySQL database. <br/> <br/> So we can confirm that the root cause of the incident that affected the 'discount' service was with the MySQL database. The abnormal termination of the database caused a connection error, which then flowed back through the application. <br/> <br/> When we bring MySQL back online, it will fix the problem. |\n| **Screenshot** | <br/> ![calls](./images/calls_600.png) |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>5 - Confirm incident resolution was successful</summary>\n<br/>\n\n| **5.1** | **Metrics for the robot shop have returned to normal** |\n| :--- | :--- |\n| **Actions** | Navigate to **Applications** in the sidebar menu, choose **Robot Shop**, and click on the **Summary** tab<br/><br/> **Note:** <br/><br/> • You should see that the call volume has increased, the number of erroneous calls decreased, and latency also decreased. <br/> • If you're giving the demo in real time, the incident should have reset itself by the time you're done demo'ing. If not, this part can be skipped. <br/> • If you set the timeframe at the beginning of the demo, you can set it again to begin at 0:30 minutes past the hour and end at 0:45 minutes past the hour. |\n| **Narration** | Now that MySQL is working again, we can go back and confirm that the problems with the robot shop have been repaired. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Screenshot** | <br/> ![robot_shop_summary_end](./images/robot_shop_summary_end_600.png) |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>Summary</summary>\n<br/>\n\nNow, we can see that the metrics for the robot shop have returned to normal: the call volume has increased again, the erroneous call rate has decreased, and latency has decreased.\n\n<br/>\n\nThe Dependencies tab is also showing that all the services are behaving normally.\n\n<br/>\n\nWe've fixed the problem with the robot shop and restored normal service!\n\n<br/>\n\nHopefully, you've seen that Instana can help make the process of identifying problems and finding the root cause of those problems very frictionless. Since Instana automates so many of the manual and labor-intensive aspects of the process, you can focus on getting other work done and not worry about instrumenting observability or constantly monitoring for problems. And when problems do arise, all the trace data is there at your fingertips to dig into.\n\n<br/>\n\nI'm happy to take any questions or go back to any part of the demo.\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n***\n","type":"Mdx","contentDigest":"e05d131b8369a4a52558834d765b1815","owner":"gatsby-plugin-mdx","counter":1541},"frontmatter":{"title":"Observability 300-level live demo","description":"Observability 300-level live demo","tabs":["Demo preparation","Demo script"]},"exports":{},"rawBody":"---\ntitle: Observability\n  300-level live demo\ndescription: Observability 300-level live demo\ntabs: [ 'Demo preparation', 'Demo script']\n---\n\nexport const Title = () => (\n  <span>\n    Observability <br/> 300-level live demo\n  </span> );\n\n![banner](./images/AIOps_Observability_300_Script.jpg)\n\n<span id=\"top\"></span>\n\n<details>\n\n<summary>Introduction</summary>\n<br/>\n\n| **Starting point** | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| :--- | :--- |\n| **Actions** | From sidebar menu, click on **Applications**<br/><br/>Choose **Robot Shop** <br/><br/>Click on the **Dependencies** tab |\n| **Narration** | In this demo, I'll show how IBM Instana helps quickly identify, debug, and resolve an incident in a microservices-based application. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Screenshot** | <br/> ![robot_shop_starting_point](./images/robot_shop_starting_point_600.png) |\n\n<br/>\n\n| **Context of demo:** | **Robot shop application** |\n| :--- | :--- |\n| **Action** | Hover over a few of the icons to show info on what technology they are built on |\n| **Narration** | To set the context, our application is called Stan's Robot Shop, and it uses various technologies such as Java, Python, and MySQL.<br/><br/>This is a visualization of all the dependencies within the robot shop application. Instana automatically discovered the relationships between the services and correlated them into this dynamic graph. We can see how requests are moving through the application in real time. Instana is able to do this because it tracks every request that flows through the application.<br/><br/>We can tell there are some problems with the application because several of the services are highlighted in yellow and red.<br/><br/>But, you wouldn't normally be looking at the dashboard when something like this happens, so let me walk you through what it looks like from the SRE/IT operator's point of view when an incident occurs. |\n| **Screenshot** | <br/> ![robot_shop_starting_point](./images/robot_shop_starting_point_600.png) |\n\n<br/>\n\n(Printer-ready PDF of demo script <a href=\"./300-Observability-Demo-Script.pdf\" target=\"_blank\" rel=\"noreferrer\">here</a>)\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>1 - Getting an incident alert</summary>\n<br/>\n\n| **1.1** | **Automatically assessing events and alerts** |\n| :--- | :--- |\n| **Action** | Click on the **Events** icon (triangle) in sidebar menu &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Narration** | We've just gotten an alert from Instana that there has been a sudden increase in erroneous calls on our 'discount' service, which is part of the robot shop application. <br/> <br/> Although I don't have it connected right now, the alert would show up via one of the configurable alert channels, like email, Microsoft Teams, Slack, and many others (<a href=\"https://www.instana.com/docs/events_alerts/alert-channels\" target=\"_blank\" rel=\"noreferrer\">full list</a>). <br/><br/>It's important to note here that you're not getting alerts for just anything. Instana automatically groups related events and issues into incidents. It determines that events and/or issues are related using the dynamic dependency graph that we just looked at. And Instana continuously assesses the groups of events and issues to determine which ones are impacting end users or posing an imminent risk of impacting end users. Those are the ones that Instana will alert on, so as a SRE/IT operator, you will not be interrupted constantly for things that are not very important. <br/><br/> Let's go into the details for this incident. |\n| **Screenshot** | <br/> ![sidebar_menu](./images/sidebar_menu_600.png) |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>2 - Inspect auto-correlated incident details</summary>\n<br/>\n\n| **2.1** | **Gather information from incident detail page** |\n| :--- | :--- |\n| **Action** | Click on the incident called **Sudden increase in the number of erroneous calls** on the 'discount' service &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Narration** | Instana recognized that the sudden increase in the number of erroneous calls was something important to alert on, so we did not have to do any configuration or set thresholds in order to get this alert. We get key information right away when we come into this incident detail page. There's a timeline of the incident, the event that triggered Instana to create the incident, and all of the related events. |\n| **Screenshots** | <br/> ![event_page](./images/event_page_600.png) <br/> <br/> <br/> ![incident_details_screen](./images/incident_details_screen_600.png) |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>3 - Debugging the incident by inspecting calls</summary>\n<br/>\n\n| **3.1** | ** Understand the incident**  |\n| :--- | :--- |\n| **Actions** | Click on the event that says **Sudden increase in the number of erroneous calls**<br/><br/>Then, click **Analyze Calls** &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Narration** | By inspecting the related events, it looks like the abnormal termination of the MySQL database caused the problem. We can go into more detail about each call that failed to connect to the database, by choosing the event that relates to the sudden increase in erroneous calls, and going to analyze calls. Going into the actual trace for a request that resulted in an error will help us confirm that MySQL is really the source of the incident.|\n| **Screenshot** | <br/> ![events](./images/events_600.png) |\n\n<br/>\n\n| **3.2** | **Examine details** |\n| :--- | :--- |\n| **Actions** | Click on the endpoint named **CONNECT**<br/><br/> Then, click on the first call (also named **CONNECT**) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Narration** | All the calls are grouped by endpoint. There is only one endpoint here, but if there were multiple, you'd see a list here. Endpoints are automatically discovered and mapped by Instana. <br/> <br/> We can go into the details for each erroneous call to MySQL via this endpoint (CONNECT). |\n| **Screenshot** | <br/> ![endpoint_connect](./images/endpoint_connect_600.png) |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>4 - Drill down with end-to-end traces</summary>\n<br/>\n\n| **4.1** | **View call via visual dashboard** |\n| :--- | :--- |\n| **Action** | -- |\n| **Narration** | Let's take a look at the first call in the list. Clicking on an individual call takes us to a view of the call in context of the end-to-end trace. We can see where the request began and each call that was made along the way. The timeline view gives a quick overview of the time spent on each span, as well as key performance indicators, such as the number of erroneous calls in this trace, the number of warning logs, and total latency.<br/><br/>Everything is presented in an easy-to-navigate visual dashboard, so we can drill into increasingly detailed information to pinpoint the problem, without using multiple tools or navigating back and forth to lots of dashboards. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Screenshot** | <br/> ![call_timeline](./images/call_timeline_600.png) |\n\n<br/>\n\n| **4.2** | **Understand the impact and source of the incident** |\n| :--- | :--- |\n| **Action** | Scroll down to the section labeled **Calls** &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Narration** | In the call stack, we can click on each span to see more information, including the complete stack trace. We can see the source, in this case the 'discount' service, and [scroll down] the destination, which in this case is CONNECT of MySQL. <br/> <br/> It's useful to have this context because we can easily see how the calls go from one service to another, just by clicking on them. We can also see how the error (red triangle) propagated up the call stack, in this case beginning with the MySQL database. <br/> <br/> So we can confirm that the root cause of the incident that affected the 'discount' service was with the MySQL database. The abnormal termination of the database caused a connection error, which then flowed back through the application. <br/> <br/> When we bring MySQL back online, it will fix the problem. |\n| **Screenshot** | <br/> ![calls](./images/calls_600.png) |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>5 - Confirm incident resolution was successful</summary>\n<br/>\n\n| **5.1** | **Metrics for the robot shop have returned to normal** |\n| :--- | :--- |\n| **Actions** | Navigate to **Applications** in the sidebar menu, choose **Robot Shop**, and click on the **Summary** tab<br/><br/> **Note:** <br/><br/> • You should see that the call volume has increased, the number of erroneous calls decreased, and latency also decreased. <br/> • If you're giving the demo in real time, the incident should have reset itself by the time you're done demo'ing. If not, this part can be skipped. <br/> • If you set the timeframe at the beginning of the demo, you can set it again to begin at 0:30 minutes past the hour and end at 0:45 minutes past the hour. |\n| **Narration** | Now that MySQL is working again, we can go back and confirm that the problems with the robot shop have been repaired. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Screenshot** | <br/> ![robot_shop_summary_end](./images/robot_shop_summary_end_600.png) |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>Summary</summary>\n<br/>\n\nNow, we can see that the metrics for the robot shop have returned to normal: the call volume has increased again, the erroneous call rate has decreased, and latency has decreased.\n\n<br/>\n\nThe Dependencies tab is also showing that all the services are behaving normally.\n\n<br/>\n\nWe've fixed the problem with the robot shop and restored normal service!\n\n<br/>\n\nHopefully, you've seen that Instana can help make the process of identifying problems and finding the root cause of those problems very frictionless. Since Instana automates so many of the manual and labor-intensive aspects of the process, you can focus on getting other work done and not worry about instrumenting observability or constantly monitoring for problems. And when problems do arise, all the trace data is there at your fingertips to dig into.\n\n<br/>\n\nI'm happy to take any questions or go back to any part of the demo.\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n***\n","fileAbsolutePath":"/home/runner/work/platinum-demos/platinum-demos/src/pages/300-watson-aiops-observability/demo-script.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}