{"componentChunkName":"component---src-pages-300-watson-aiops-proactive-incident-resolution-demo-script-mdx","path":"/300-watson-aiops-proactive-incident-resolution/demo-script/","result":{"pageContext":{"frontmatter":{"title":"Proactive incident resolution 300-level live demo","description":"Proactive incident resolution 300-level live demo","tabs":["Demo preparation","Demo script"]},"relativePagePath":"/300-watson-aiops-proactive-incident-resolution/demo-script.mdx","titleType":"page","MdxNode":{"id":"6ab67606-2d96-5314-ab6e-9f804a7dd630","children":[],"parent":"da6cde59-e115-5b1c-9fc1-aece07a584d5","internal":{"content":"---\ntitle: Proactive incident resolution\n  300-level live demo\ndescription: Proactive incident resolution 300-level live demo\ntabs: [ 'Demo preparation', 'Demo script']\n---\n\nexport const Title = () => (\n  <span>\n    Proactive incident resolution <br/> 300-level live demo\n  </span> );\n\n![banner](./images/proactive-incident-resolution-300-script-github-banner.jpg)\n\n<span id=\"place1\"></span>\n\n<details>\n<summary>Introduction</summary>\n<br/>\n\nIn this demo, I’ll show you how Cloud Pak for Watson AIOps helps SREs and IT Ops proactively identify, diagnose, and resolve incidents across mission-critical workloads.\n\n<br/>\n\nYou’ll see how: <br/>\n•\tWatson AIOps intelligently correlates multiple disparate sources of information such as logs, metrics, events, tickets and topology; <br/>\n•\tAll of this information is condensed and presented in actionable alerts instead of large quantities of unrelated alerts, and; <br/>\n•\tYou can resolve a problem within seconds to minutes of being notified using Watson AIOps’ automation capabilities <br/>\n\n<br/>\n\nWe will be using an application called Quote of the Day, which will serve as a proxy for any type of app. This is a content delivery app that serves up random quotations. The application is built on a microservices architecture, and the services are running on Kubernetes.\n\n<br/><br/>\n\n(Printer-ready PDF of demo script <a href=\"./300-Incident-Resolution-Demo-Script.pdf\" target=\"_blank\" rel=\"noreferrer\">here</a>)\n<br/>\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>1 - Simulating a failure</summary>\n\n<br/>\n\n| **1.1** | **Navigate to the Anomaly Generator and input sources** |\n| :--- | :--- |\n| **Actions** | Navigate to the **Anomaly Generator**<br/><br/>Choose **Rating service failure**<br/><br/>Click **Start** |\n| **Narration** | To see how this all works, I’m going to generate an anomaly in our application. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Screenshot** | <br/> ![Quote of the Day app](./images/quote-app.png) |\n| **Actions** | The anomaly generator will take a minute or so to start showing alerts in Slack<br/><br/> While it is running, navigate to the various input sources: Prometheus and ELK |\n| **Narration** | Without Watson AIOps, we would get all sorts of alerts and notifications from multiple sources when a problem occurs.<br/><br/>Prometheus would start firing alerts, but we would have to look at them manually to find out if they are related.<br/><br/>Same with ELK – we might see new types of logs or errors coming in, but again it’s time consuming to determine the relationship. Hundreds of logs are streaming in every minute, making it very difficult for a human to keep up with them.<br/><br/>As we’ll see in a moment, with Watson AIOps, all of this information gets correlated and presented all in one place. This includes recommendations for how to resolve the incident. We can take action directly from the notification and resolve the incident quickly. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Screenshots** | <br/> ![Prometheus](./images/prometheus.png) <br/>![Elastic](./images/elastic.png) |\n\n<br/><br/>\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>2 - Getting notified of an emerging problem</summary>\n\n<br/>\n\n| **2.1** | **AIOps formats notification as story and adds affected services to it** |\n| :--- | :--- |\n| **Action** | Story starts showing up in Slack |\n| **Narration** | Notifications are now appearing in Slack. We’re using Slack in this demo, but Watson AIOps also integrates with Microsoft Teams.<br/><br/>Watson AIOps formats the notifications into a “story” using AI to correlate events, metrics, alerts, and logs. Each story brings together the various notifications for all the affected services by the same underlying issue. Imagine if each piece of data presented in the story was a separate notification – we’d quickly be inundated with alerts. <br/><br/>The story is like a home base for action when a problem arises. Instead of manually correlating things across multiple different tools, it’s all right here immediately when the notification is received. <br/><br/>In addition to providing a highly contextualized view of the incident, it enables us to jump in-context to other point tools to explore further details. This helps eliminate tool silos and helps us restore service faster.<br/><br/>This story is telling us there’s a problem with the Rating service, which is one of the microservices in our Quote of the Day application.<br/><br/>In the background, the AI has done the work for us and presents us with a curated view of relevant information. It shows which services are affected: the relevant events and alerts that are indicative of the symptoms of this problem; anomalies that Watson AIOps has found in the log files; and similar incidents that have occurred in the past so we can see how they were successfully resolved. We’ll explore each of these components in more detail.<br/><br/>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Screenshot** | <br/> ![Finding root cause](./images/incident-title.png) |\n| **Action** | Additional affected services are added to the story |\n| **Narration** | When this story first appeared, the only affected service was the Rating service.<br/><br/>Watson AIOps updates the stories in real time as more information comes in and gets correlated to this story. <br/><br/>So now we can see that in addition to Rating, the Web service is also affected. This is now even more critical since the Web service is the customer-facing front-end of the application, and we need to ensure that users can still access the app.<br/><br/>We need to find the root cause and fix this problem as quickly as possible.<br/><br/>\n| **Screenshot** | <br/> ![Component](./images/component.png) |\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>3 - Determining which service caused the problem</summary>\n\n<br/>\n\n| **3.1** | **Review the notification to learn about issue** |\n| :--- | :--- |\n| **Action** | Scroll to the **Relevant events** section of the story |\n| **Narration** | We need to find out where the issue began so we can prevent it from causing cascading failures across the components of the application.<br/><br/>Instead of having to go to Prometheus or another tool to look at the alerts, we can see them right here from the notification. Watson AIOps has determined that these events are related, and provides an explanation for how it determined the relationships. We can see that there are two groups of events based on related resources and the timing of the events.<br/><br/>\n| **Screenshot** | <br/> ![Relevant events](./images/relevant-events.png) |\n| **Actions** | Click on the **View relevant events** button at the bottom of the notification<br/><br/>A pop-up will appear with the grouped events<br/>\n| **Narration** | We can inspect the grouped events right here, without searching for them in another tool.<br/><br/>It looks like the memory and CPU on the Rating service increased significantly. This is causing a significant slow-down in response time on both the Rating and Web services.<br/><br/>Based on this information, it seems that the Rating service is the source of the issue. But let’s get a bit more detail – this time from the log files.<br/><br/>\n| **Screenshots** | <br/> ![View relevant events](./images/view-relevant-events.png) <br/>![Relevant events alerts](./images/relevant-events-alerts.png) |\n| **Action** | Scroll down past the alerts to show **Log anomaly** |\n| **Narration** | Instead of needing to go to Kibana and manually sort through the hundreds or thousands of log entries that come in every minute, Watson AIOps has found several anomalies in the log files and presented them here. It trains on the log files of the application when it’s operating normally, and continually monitors for deviations from that baseline.<br/><br/>We can see that the anomalies are occurring on the Rating service, which fits with what we saw in the alerts. |\n| **Screenshot** | <br/> ![Rating service](./images/rating-service.png) |\n| **Action** | Click on **Show more** |\n| **Narration** | Watson AIOps gives us additional context on the anomaly. In this case, the “unknown_error” anomaly is telling us that Watson AIOps has never seen this type of log before (hence the “unknown”) and that the log message indicates there is some type of error. Watson AIOps is not only looking at the statistical frequency of the type of log, but it is also using Natural Language Processing to analyze the content of the log message to give additional context (in this case that there’s likely an error).<br/><br/>Watson AIOps also explains why the anomaly was flagged. It expected to see zero (0) of this type of log, but it actually saw four (4).<br/><br/>Now we know that there is an unfamiliar log coming from the Rating service, and it’s indicating an error.<br/><br/>This further reinforces what we saw with the alerts -  it looks like the Rating service is likely the root cause of this problem.<br/><br/>\n| **Screenshots** | <br/> ![anomaly](./images/anomaly.png) <br/>![anomaly 2](./images/log-anomaly-1.png) |\n| **Action** | For the 'Unknown_error' anomaly, click on **Preview logs** |\n| **Narration** | We can preview the log messages that caused Watson AIOps to find the anomaly. |\n| **Screenshots** | <br/> ![Preview logs](./images/preview-logs.png) <br/> ![Log preview](./images/log-preview.png) |\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>4 - Getting resolution recommendations</summary>\n\n<br/>\n\n| **4.1** | **Watson AIOps searches similar incidents for recommendations** |\n| :--- | :--- |\n| **Actions** | Close the log anomaly pop-up<br/><br/>Click on the **Search similar incidents** button<br/><br/>Type 'rating' into the search box |\n| **Narration** | Now that we understand a bit more about what’s going on, we need to focus on our main goal: resolving the incident as quickly as possible.<br/><br/>Watson AIOps also brings in similar incidents and information regarding how they were resolved. This enhances operational efficiency by leveraging institutional knowledge that may be time consuming to find otherwise.<br/><br/>Since the alerts seem to point to the Rating service as having the memory and CPU increases, we’ll search for incidents that happened with that service. <br/><br/>Watson AIOps found two similar incidents. The first one seems like what we’re experiencing: Rating service is overheating and slowing. Using Natural Language Processing, Watson AIOps  went through the comments on the incident and highlighted key relevant information for us, in this case that the incident was resolved by running a runbook.<br/><br/>If we can use the same runbook to resolve the current incident it will make our job that much easier and restore service even faster! |\n| **Screenshots** | <br/> ![Search similar incidents](./images/search-similar-incidents.png) <br/>![Search similar incidents - Rating](./images/search-similar-incidents-rating.png) <br/>![Search results](./images/search-results.png) |\n| **Actions** | Click on the link for the first similar incident **Rating service overheating and slowing**<br/><br/>It will open a window in GitLab<br/><br/>If needed, sort comments **newest first** |\n| **Narration** | The description for this incident sounds a lot like what we are observing right now. The Rating service is stressed, with increases in CPU, memory, and latency. It’s also affecting the Web service.<br/><br/>The comment says there is a runbook that resolved this issue. This is very useful information that may have taken a long time to find out just by searching or asking colleagues. Watson AIOps helped us find this info very quickly. |\n| **Screenshots** | <br/> ![Newest first](./images/newest-first.png) |\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>5 - Fixing the problem and restoring service</summary>\n\n<br/>\n\n| **5.1** | **View Event Manager to determine the root cause and fix the problem** |\n| :--- | :--- |\n| **Actions** | Navigate to **Event Manager** (formally known as NOI)<br/><br/>Click on **Events** in the sidebar menu<br/><br/>The top event should show a red circle (**critical**)<br/><br/>Click on the **Down** arrow to expand it |\n| **Narration** | This is the Event Manager component of Watson AIOps. Here’s the event group that we’re working on. There are four events grouped together – these are the same ones we previewed earlier from the Slack notification. There’s additional detail here, including the likelihood of each event being the root cause of the overall issue and if there is a runbook associated with the event.<br/><br/>There are three events that occurred on the Rating service. They’re all rated as 99% likely to be the root cause - meaning that the Rating service is the source of the problem.<br/><br/>It also shows us that there’s a runbook associated with these events, which is what was mentioned in the GitLab incident comment. |\n| **Screenshots** | <br/> ![Events1](./images/events1.png) <br/> ![Events2](./images/events2.png) |\n| **Actions** | Click on the second row to select it (**Summary** column: **Rating service Mem usage very high**)<br/><br/>Click on **Execute runbook**<br/><br/>Click on **Start runbook**<br/><br/>Click on **Run** |\n| **Narration** | Now we’ll execute the runbook that should resolve the underlying failure of the Rating service. That should fix the problem. |\n| **Screenshots** | <br/> ![Events3](./images/events3.png) <br/>![runbook1](./images/runbook1.png) <br/> ![Runbook2](./images/runbook2.png) |\n| **Actions** | Navigate to **Instana**<br/><br/>Click on the **Summary** tab<br/><br/>Set the time period to **last 15 minutes**, and click on the **Live** button (you will be showing the latency before, during, and after the incident) |\n| **Narration** | To check that the runbook is fixing the problem, we can look at the health metrics in Instana. We’ll set the time period so we can see data from when the incident began until now.<br/><br/>We can see that the latency of the application started increasing when the problem began. Now that we’ve run the runbook, latency is going back down. This confirms that the runbook was successful. |\n| **Screenshot** | <br/> ![runbook success](./images/runbook-success.png) |\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>6 - Closing out the incident</summary>\n\n<br/>\n\n| **6.1** | **Provide feedback on the runbook and close incident** |\n| :--- | :--- |\n| **Actions** | Go back to the Runbook screen<br/><br/>If you navigated away from it, go back with these steps:<br/><br/>Automations -> Runbooks -> click on the three dots at the far right for the **Reset Rating Service** runbook -> Choose **View history** -> Click on **Resume on most recent**<br/><br/>Scroll down to the bottom where there are stars/ratings |\n| **Narration** | We can rate the runbook to preserve the information that it was successful – this will be helpful for the next person who comes along looking at this runbook, as well as feedback to the author of the runbook. <br/><br/>This enables a collaborative approach to organically improve incident resolution over time. As the system matures it can evolve into an increasingly automated self-healing system. |\n| **Screenshot** | <br/> ![runbook worked](./images/runbook-worked.png) |\n| **Actions** | Navigate to **Slack**<br/><br/>Add details about the resolution by clicking the **Add to story** button |\n| **Narration** | Now we are ready to close out the incident. <br/><br/>We found the runbook because it was mentioned in the similar incident, so we want to preserve that information. To do so, we can click **add to story** and Watson AIOps adds a comment with the information about the similar incident.<br/><br/> This keeps all the information in one place so it’s easy to find later or if another similar incident occurs in the future. |\n| **Screenshot** | <br/> ![Search results Story 695](./images/search-results-story-695.png) |\n| **Actions** | At the bottom of the story notification, click on **Acknowledge**<br/><br/>Choose **Keep in current channel** and **Closed**<br/><br/> Optionally add a comment, such as 'Executed runbook to fix issue,' and then click on **Acknowledge** |\n| **Narration** | Then we will acknowledge the incident and mark it as closed. Watson AIOps archives all the information from the incident in the comments of the original message. So we can go back to it whenever needed to see exactly what happened and how it was fixed. |\n| **Screenshot** | <br/> ![acknowledge](./images/acknowledge.png) <br/> ![closed](./images/closed.png) |\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>Summary</summary>\n\n<br/>\nIn this demo we have demonstrated how Watson AIOps enables you to avoid business impacting outages by applying AI to data gathered from your existing disparate tools. It helps you quickly determine the root causes of a failure and proactively alleviates outages – therefore minimizing the business impact of these IT issues on revenue or client experience.<br/><br/>\nThe anomaly detection capabilities alerts you early to potential issues enabling the SRE take quick remedial actions. The intelligent event analytics examines logs, metrics, tickets and topology and provides a useful correlation that would otherwise be very challenging. Lastly, the AI-driven remediation based on recommendations of previous incidents accelerates problem resolution and significantly reduces the mean-time-to-repair.\n<br/><br/>\n\n**[Go to top](#place1)**\n\n</details>\n","type":"Mdx","contentDigest":"a59c04f50a89a66c766e76e9799d21ef","owner":"gatsby-plugin-mdx","counter":2079},"frontmatter":{"title":"Proactive incident resolution 300-level live demo","description":"Proactive incident resolution 300-level live demo","tabs":["Demo preparation","Demo script"]},"exports":{},"rawBody":"---\ntitle: Proactive incident resolution\n  300-level live demo\ndescription: Proactive incident resolution 300-level live demo\ntabs: [ 'Demo preparation', 'Demo script']\n---\n\nexport const Title = () => (\n  <span>\n    Proactive incident resolution <br/> 300-level live demo\n  </span> );\n\n![banner](./images/proactive-incident-resolution-300-script-github-banner.jpg)\n\n<span id=\"place1\"></span>\n\n<details>\n<summary>Introduction</summary>\n<br/>\n\nIn this demo, I’ll show you how Cloud Pak for Watson AIOps helps SREs and IT Ops proactively identify, diagnose, and resolve incidents across mission-critical workloads.\n\n<br/>\n\nYou’ll see how: <br/>\n•\tWatson AIOps intelligently correlates multiple disparate sources of information such as logs, metrics, events, tickets and topology; <br/>\n•\tAll of this information is condensed and presented in actionable alerts instead of large quantities of unrelated alerts, and; <br/>\n•\tYou can resolve a problem within seconds to minutes of being notified using Watson AIOps’ automation capabilities <br/>\n\n<br/>\n\nWe will be using an application called Quote of the Day, which will serve as a proxy for any type of app. This is a content delivery app that serves up random quotations. The application is built on a microservices architecture, and the services are running on Kubernetes.\n\n<br/><br/>\n\n(Printer-ready PDF of demo script <a href=\"./300-Incident-Resolution-Demo-Script.pdf\" target=\"_blank\" rel=\"noreferrer\">here</a>)\n<br/>\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>1 - Simulating a failure</summary>\n\n<br/>\n\n| **1.1** | **Navigate to the Anomaly Generator and input sources** |\n| :--- | :--- |\n| **Actions** | Navigate to the **Anomaly Generator**<br/><br/>Choose **Rating service failure**<br/><br/>Click **Start** |\n| **Narration** | To see how this all works, I’m going to generate an anomaly in our application. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Screenshot** | <br/> ![Quote of the Day app](./images/quote-app.png) |\n| **Actions** | The anomaly generator will take a minute or so to start showing alerts in Slack<br/><br/> While it is running, navigate to the various input sources: Prometheus and ELK |\n| **Narration** | Without Watson AIOps, we would get all sorts of alerts and notifications from multiple sources when a problem occurs.<br/><br/>Prometheus would start firing alerts, but we would have to look at them manually to find out if they are related.<br/><br/>Same with ELK – we might see new types of logs or errors coming in, but again it’s time consuming to determine the relationship. Hundreds of logs are streaming in every minute, making it very difficult for a human to keep up with them.<br/><br/>As we’ll see in a moment, with Watson AIOps, all of this information gets correlated and presented all in one place. This includes recommendations for how to resolve the incident. We can take action directly from the notification and resolve the incident quickly. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Screenshots** | <br/> ![Prometheus](./images/prometheus.png) <br/>![Elastic](./images/elastic.png) |\n\n<br/><br/>\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>2 - Getting notified of an emerging problem</summary>\n\n<br/>\n\n| **2.1** | **AIOps formats notification as story and adds affected services to it** |\n| :--- | :--- |\n| **Action** | Story starts showing up in Slack |\n| **Narration** | Notifications are now appearing in Slack. We’re using Slack in this demo, but Watson AIOps also integrates with Microsoft Teams.<br/><br/>Watson AIOps formats the notifications into a “story” using AI to correlate events, metrics, alerts, and logs. Each story brings together the various notifications for all the affected services by the same underlying issue. Imagine if each piece of data presented in the story was a separate notification – we’d quickly be inundated with alerts. <br/><br/>The story is like a home base for action when a problem arises. Instead of manually correlating things across multiple different tools, it’s all right here immediately when the notification is received. <br/><br/>In addition to providing a highly contextualized view of the incident, it enables us to jump in-context to other point tools to explore further details. This helps eliminate tool silos and helps us restore service faster.<br/><br/>This story is telling us there’s a problem with the Rating service, which is one of the microservices in our Quote of the Day application.<br/><br/>In the background, the AI has done the work for us and presents us with a curated view of relevant information. It shows which services are affected: the relevant events and alerts that are indicative of the symptoms of this problem; anomalies that Watson AIOps has found in the log files; and similar incidents that have occurred in the past so we can see how they were successfully resolved. We’ll explore each of these components in more detail.<br/><br/>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n| **Screenshot** | <br/> ![Finding root cause](./images/incident-title.png) |\n| **Action** | Additional affected services are added to the story |\n| **Narration** | When this story first appeared, the only affected service was the Rating service.<br/><br/>Watson AIOps updates the stories in real time as more information comes in and gets correlated to this story. <br/><br/>So now we can see that in addition to Rating, the Web service is also affected. This is now even more critical since the Web service is the customer-facing front-end of the application, and we need to ensure that users can still access the app.<br/><br/>We need to find the root cause and fix this problem as quickly as possible.<br/><br/>\n| **Screenshot** | <br/> ![Component](./images/component.png) |\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>3 - Determining which service caused the problem</summary>\n\n<br/>\n\n| **3.1** | **Review the notification to learn about issue** |\n| :--- | :--- |\n| **Action** | Scroll to the **Relevant events** section of the story |\n| **Narration** | We need to find out where the issue began so we can prevent it from causing cascading failures across the components of the application.<br/><br/>Instead of having to go to Prometheus or another tool to look at the alerts, we can see them right here from the notification. Watson AIOps has determined that these events are related, and provides an explanation for how it determined the relationships. We can see that there are two groups of events based on related resources and the timing of the events.<br/><br/>\n| **Screenshot** | <br/> ![Relevant events](./images/relevant-events.png) |\n| **Actions** | Click on the **View relevant events** button at the bottom of the notification<br/><br/>A pop-up will appear with the grouped events<br/>\n| **Narration** | We can inspect the grouped events right here, without searching for them in another tool.<br/><br/>It looks like the memory and CPU on the Rating service increased significantly. This is causing a significant slow-down in response time on both the Rating and Web services.<br/><br/>Based on this information, it seems that the Rating service is the source of the issue. But let’s get a bit more detail – this time from the log files.<br/><br/>\n| **Screenshots** | <br/> ![View relevant events](./images/view-relevant-events.png) <br/>![Relevant events alerts](./images/relevant-events-alerts.png) |\n| **Action** | Scroll down past the alerts to show **Log anomaly** |\n| **Narration** | Instead of needing to go to Kibana and manually sort through the hundreds or thousands of log entries that come in every minute, Watson AIOps has found several anomalies in the log files and presented them here. It trains on the log files of the application when it’s operating normally, and continually monitors for deviations from that baseline.<br/><br/>We can see that the anomalies are occurring on the Rating service, which fits with what we saw in the alerts. |\n| **Screenshot** | <br/> ![Rating service](./images/rating-service.png) |\n| **Action** | Click on **Show more** |\n| **Narration** | Watson AIOps gives us additional context on the anomaly. In this case, the “unknown_error” anomaly is telling us that Watson AIOps has never seen this type of log before (hence the “unknown”) and that the log message indicates there is some type of error. Watson AIOps is not only looking at the statistical frequency of the type of log, but it is also using Natural Language Processing to analyze the content of the log message to give additional context (in this case that there’s likely an error).<br/><br/>Watson AIOps also explains why the anomaly was flagged. It expected to see zero (0) of this type of log, but it actually saw four (4).<br/><br/>Now we know that there is an unfamiliar log coming from the Rating service, and it’s indicating an error.<br/><br/>This further reinforces what we saw with the alerts -  it looks like the Rating service is likely the root cause of this problem.<br/><br/>\n| **Screenshots** | <br/> ![anomaly](./images/anomaly.png) <br/>![anomaly 2](./images/log-anomaly-1.png) |\n| **Action** | For the 'Unknown_error' anomaly, click on **Preview logs** |\n| **Narration** | We can preview the log messages that caused Watson AIOps to find the anomaly. |\n| **Screenshots** | <br/> ![Preview logs](./images/preview-logs.png) <br/> ![Log preview](./images/log-preview.png) |\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>4 - Getting resolution recommendations</summary>\n\n<br/>\n\n| **4.1** | **Watson AIOps searches similar incidents for recommendations** |\n| :--- | :--- |\n| **Actions** | Close the log anomaly pop-up<br/><br/>Click on the **Search similar incidents** button<br/><br/>Type 'rating' into the search box |\n| **Narration** | Now that we understand a bit more about what’s going on, we need to focus on our main goal: resolving the incident as quickly as possible.<br/><br/>Watson AIOps also brings in similar incidents and information regarding how they were resolved. This enhances operational efficiency by leveraging institutional knowledge that may be time consuming to find otherwise.<br/><br/>Since the alerts seem to point to the Rating service as having the memory and CPU increases, we’ll search for incidents that happened with that service. <br/><br/>Watson AIOps found two similar incidents. The first one seems like what we’re experiencing: Rating service is overheating and slowing. Using Natural Language Processing, Watson AIOps  went through the comments on the incident and highlighted key relevant information for us, in this case that the incident was resolved by running a runbook.<br/><br/>If we can use the same runbook to resolve the current incident it will make our job that much easier and restore service even faster! |\n| **Screenshots** | <br/> ![Search similar incidents](./images/search-similar-incidents.png) <br/>![Search similar incidents - Rating](./images/search-similar-incidents-rating.png) <br/>![Search results](./images/search-results.png) |\n| **Actions** | Click on the link for the first similar incident **Rating service overheating and slowing**<br/><br/>It will open a window in GitLab<br/><br/>If needed, sort comments **newest first** |\n| **Narration** | The description for this incident sounds a lot like what we are observing right now. The Rating service is stressed, with increases in CPU, memory, and latency. It’s also affecting the Web service.<br/><br/>The comment says there is a runbook that resolved this issue. This is very useful information that may have taken a long time to find out just by searching or asking colleagues. Watson AIOps helped us find this info very quickly. |\n| **Screenshots** | <br/> ![Newest first](./images/newest-first.png) |\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>5 - Fixing the problem and restoring service</summary>\n\n<br/>\n\n| **5.1** | **View Event Manager to determine the root cause and fix the problem** |\n| :--- | :--- |\n| **Actions** | Navigate to **Event Manager** (formally known as NOI)<br/><br/>Click on **Events** in the sidebar menu<br/><br/>The top event should show a red circle (**critical**)<br/><br/>Click on the **Down** arrow to expand it |\n| **Narration** | This is the Event Manager component of Watson AIOps. Here’s the event group that we’re working on. There are four events grouped together – these are the same ones we previewed earlier from the Slack notification. There’s additional detail here, including the likelihood of each event being the root cause of the overall issue and if there is a runbook associated with the event.<br/><br/>There are three events that occurred on the Rating service. They’re all rated as 99% likely to be the root cause - meaning that the Rating service is the source of the problem.<br/><br/>It also shows us that there’s a runbook associated with these events, which is what was mentioned in the GitLab incident comment. |\n| **Screenshots** | <br/> ![Events1](./images/events1.png) <br/> ![Events2](./images/events2.png) |\n| **Actions** | Click on the second row to select it (**Summary** column: **Rating service Mem usage very high**)<br/><br/>Click on **Execute runbook**<br/><br/>Click on **Start runbook**<br/><br/>Click on **Run** |\n| **Narration** | Now we’ll execute the runbook that should resolve the underlying failure of the Rating service. That should fix the problem. |\n| **Screenshots** | <br/> ![Events3](./images/events3.png) <br/>![runbook1](./images/runbook1.png) <br/> ![Runbook2](./images/runbook2.png) |\n| **Actions** | Navigate to **Instana**<br/><br/>Click on the **Summary** tab<br/><br/>Set the time period to **last 15 minutes**, and click on the **Live** button (you will be showing the latency before, during, and after the incident) |\n| **Narration** | To check that the runbook is fixing the problem, we can look at the health metrics in Instana. We’ll set the time period so we can see data from when the incident began until now.<br/><br/>We can see that the latency of the application started increasing when the problem began. Now that we’ve run the runbook, latency is going back down. This confirms that the runbook was successful. |\n| **Screenshot** | <br/> ![runbook success](./images/runbook-success.png) |\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>6 - Closing out the incident</summary>\n\n<br/>\n\n| **6.1** | **Provide feedback on the runbook and close incident** |\n| :--- | :--- |\n| **Actions** | Go back to the Runbook screen<br/><br/>If you navigated away from it, go back with these steps:<br/><br/>Automations -> Runbooks -> click on the three dots at the far right for the **Reset Rating Service** runbook -> Choose **View history** -> Click on **Resume on most recent**<br/><br/>Scroll down to the bottom where there are stars/ratings |\n| **Narration** | We can rate the runbook to preserve the information that it was successful – this will be helpful for the next person who comes along looking at this runbook, as well as feedback to the author of the runbook. <br/><br/>This enables a collaborative approach to organically improve incident resolution over time. As the system matures it can evolve into an increasingly automated self-healing system. |\n| **Screenshot** | <br/> ![runbook worked](./images/runbook-worked.png) |\n| **Actions** | Navigate to **Slack**<br/><br/>Add details about the resolution by clicking the **Add to story** button |\n| **Narration** | Now we are ready to close out the incident. <br/><br/>We found the runbook because it was mentioned in the similar incident, so we want to preserve that information. To do so, we can click **add to story** and Watson AIOps adds a comment with the information about the similar incident.<br/><br/> This keeps all the information in one place so it’s easy to find later or if another similar incident occurs in the future. |\n| **Screenshot** | <br/> ![Search results Story 695](./images/search-results-story-695.png) |\n| **Actions** | At the bottom of the story notification, click on **Acknowledge**<br/><br/>Choose **Keep in current channel** and **Closed**<br/><br/> Optionally add a comment, such as 'Executed runbook to fix issue,' and then click on **Acknowledge** |\n| **Narration** | Then we will acknowledge the incident and mark it as closed. Watson AIOps archives all the information from the incident in the comments of the original message. So we can go back to it whenever needed to see exactly what happened and how it was fixed. |\n| **Screenshot** | <br/> ![acknowledge](./images/acknowledge.png) <br/> ![closed](./images/closed.png) |\n\n**[Go to top](#place1)**\n\n</details>\n\n<details>\n\n<summary>Summary</summary>\n\n<br/>\nIn this demo we have demonstrated how Watson AIOps enables you to avoid business impacting outages by applying AI to data gathered from your existing disparate tools. It helps you quickly determine the root causes of a failure and proactively alleviates outages – therefore minimizing the business impact of these IT issues on revenue or client experience.<br/><br/>\nThe anomaly detection capabilities alerts you early to potential issues enabling the SRE take quick remedial actions. The intelligent event analytics examines logs, metrics, tickets and topology and provides a useful correlation that would otherwise be very challenging. Lastly, the AI-driven remediation based on recommendations of previous incidents accelerates problem resolution and significantly reduces the mean-time-to-repair.\n<br/><br/>\n\n**[Go to top](#place1)**\n\n</details>\n","fileAbsolutePath":"/home/runner/work/platinum-demos/platinum-demos/src/pages/300-watson-aiops-proactive-incident-resolution/demo-script.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}