{"componentChunkName":"component---src-pages-300-integration-dent-deletion-demo-script-mdx","path":"/300-integration-dent-deletion/demo-script/","result":{"pageContext":{"frontmatter":{"title":"Automating deployment of multi-style integrations <br/> 300-level live demo 300-level live demo","description":"Dent deletion 300-level live demo","tabs":["Demo preparation","Demo script"]},"relativePagePath":"/300-integration-dent-deletion/demo-script.mdx","titleType":"page","MdxNode":{"id":"aab0169d-6815-5202-993a-4598c33c78d5","children":[],"parent":"65ed67de-29c2-5ac3-b6c1-6877c259fe19","internal":{"content":"---\ntitle: Automating deployment of multi-style integrations <br/> 300-level live demo\n  300-level live demo\ndescription: Dent deletion 300-level live demo\ntabs: [ 'Demo preparation', 'Demo script']\n---\n\nexport const Title = () => (\n  <span>\n      Automating deployment of multi-style integrations <br/> 300-level live demo\n  </span> );\n\n![PENDING Dent Deletion banner](./images/Predictive-Decisioning-300-Script-GitHub-banner-12-15-21.jpg)\n\n<span id=\"top\"></span>\n\n<details>\n\n<summary>Introduction</summary>\n\n<br/>\n\nWe’re going to demonstrate how to automate deployment of a complex integration solution using a pipeline. This enables faster, more frequent delivery of changes into production and improves deployment confidence. We’re also going to show how container-based platforms enable operational consistency and automation (across those capabilities), simplifying administration of an environment.\n\n<br/>\n\nOur example scenario features an insurance broker gathering insurance quotes from multiple companies and providing an aggregated list to customers via an API. This involves multiple integration styles, including application integration, API management, and messaging. Our goal is automated deployment and operations for the process.\n\n<br/>\n\nLet's begin with a simple deployment of a single integration: an integration that retrieves a quote from an insurer. We’re going to deploy this integration using cloud native techniques on containers. You will see that you don’t need to know anything about containers, Kubernetes or OpenShift (Red Hat’s Kubernetes platform) to  deploy a simple integration.  \n\n<br/>\n\nLater, we will explore a more complex solution, with multiple integration abilities that we want to deploy together - including integration flows, queues, and managed APIs.\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>1 - Accessing the Environment</summary>\n\n<br/>\n\n| **1.1** | **Log into Cloud Pak for Integration** |\n| :--- | :--- |\n| **Narration** | Let’s see IBM Cloud Pak for Integration in action. |\n| **Action** &nbsp; 1.1.1 | Open Cloud Pak for Integration and click **IBM provided credentials (admin only)**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-1-1-1.png\" width=\"800\" /> |\n| **Narration** | Let's log in to a cloud version on IBM Cloud. |\n| **Action** &nbsp; 1.1.2 | **Log in** with your admin **username** and **password**.  <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-1-1-2.png\" width=\"800\" /> |\n\n| **1.2** | **View the Cloud Pak for Integration home screen** |\n| :--- | :--- |\n| **Narration** | This is the IBM Cloud Pak for Integration home screen, which shows all the capabilities of the pak in one place. Specialized integration capabilities for API management, application integration, messaging, and more, are built on top of powerful automation services. Let’s see the integration capabilities available. |\n| **Action** &nbsp; 1.2.1 | From the **Home Page**, click **Integration Capabilities** (if necessary close the tour dialog). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-1-2-1.png\" width=\"800\" /> |\n\n| **1.3** | **View the Cloud Pak for Integration home screen (same as step 1.2)** |\n| :--- | :--- |\n| **Narration** | You are able to access all the integration capabilities your team needs through a single interface, including API management, application integration, enterprise messaging, events, and high-speed transfer. To automate customer interactions in this demo, we will use App Connect for application integration, API Connect for API management, and the Message Queue for Enterprise Messaging.<br/><br/>Let’s open our App Connect Dashboard. |\n| **Action** &nbsp; 1.3.1 | On the **Integration Capabilities** page, click the **Integration dashboard** (ace-dashboard-demo). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-1-3-1.png\" width=\"800\" /><br/> |\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>2 - Deploying your Integration </summary>\n\n<br/>\n\n| **2.1** | **Create an Integration Server** |\n| :--- | :--- |\n| **Narration** | We’ll create an integration server our new integration deplyment. Each integration server is deployed in a separate container. |\n| **Action** &nbsp; 2.1.1 | Click **Create a server**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-1-1.png\" width=\"800\" /> |\n\n| **2.2** | **Import the BAR file** |\n| :--- | :--- |\n| **Narration** | In this example, we're going to deploy an integration that we've already created in the App Connect toolkit. We can simply drag and drop it into the console straight from the file system. If we’ve loaded the bar file before, it will be available from an internal asset repository. |\n| **Action** &nbsp; 2.2.1 | Select **Quick start toolkit integration** (1). Click **Next** (2). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-2-1.png\" width=\"800\" /> |\n| **Action** &nbsp; 2.2.2 | Upload the **HTTPEchoApp.bar** BAR file (1) that you downloaded during demo preparation. Click **Next** (2). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-2-2.png\" width=\"800\" /> |\n\n| **2.3** | **Configure your integration server** |\n| :--- | :--- |\n| **Narration** | We are then asked if we would like to apply any specific configuration information to this particular deployment, such as user credentials or certificates that are required to integrate to a particular backend system. In our case, this might include credentials to the downstream insurance quotation engine. These are held within Kubernetes in standard mechanisms known as ConfigMaps and Secrets, with visibilility limited to those with correct permissions to view, upload and change credentials. <br/><br/>(Having such a clear separation of things that change per environment (e.g. credentials) and things that don’t (e.g. integration code) improves our deployment confidence, and enables governed usage of sensitive information. We reduce errors due to “drift” between the configuration of one environment and another, and reduce the chance of credentials getting into the wrong hands. This is a good practice generally, and container technology makes this separation of concerns simpler and clearer. <br/><br/>This is particularly important in integration scenarios - such as our insurance brokerage example - which have credentials enabling access to multiple third-party insurers. Those systems may provide access to sensitive personal data. As an exmaple, the insurers may be charging the broker for access to their API, or conversely, providing commission on quotes that are followed up. Misuse of the credentials could have all manner of undesired effects.) |\n| **Action** &nbsp; 2.3.1 | Click **Next**.<br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-3-1.png\" width=\"800\" /> |\n| **Narration** | We are then asked to fill in a form, a simple way of populating a file that will be used during deployment. The file has the details about how your integration will be deployed and operated. We can decide things such as the version of the runtime that we want the integration to run against, and that is specific to this particular integration. Another integration might be running against a different version that it was tested against. <br/><br/>This is also where we decide how many replicas of the integration container we want. In a traditional installation, all integrations would inherit the same characteristics of the high availability pair of the centralised servers. In our scenario, our customers might be particularly sensitive to outages, so we might decide to minimize the availabiltiy impact of any runtime failures by increasing the number of container replicas to \"3\", or \"5, or \"7\".<br/><br/>We can also change the amount of memory assigned to the container if we knew the data model for the insurance quotation was particularly large and therefore memory intensive. |\n| **Action** &nbsp; 2.3.2 | Name the integration server **simple-echo-app** (1). Change the **Replicas** number to **3** (2). Open the **YAML editor** (3).<br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-3-2.png\" width=\"800\" /> |\n| **Narration** | This form is just a graphical way of editing the yaml formatted deployment properties file. You can see it's doing nothing more than updating that text file, and we can see our number of replicas has been set to \"3\". This file is known as the “custom resource definition” for the integration, but again, you don’t actually need to know that to deploy from here. This is the file that you would use to instantiate this integration using command line tools, or by calling one of the OpenShift APIs, or from a pipeline - as we will do later in this demonstration. |\n| **Action** &nbsp; 2.3.3 | Open the **Common settings** tab. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-3-3.png\" width=\"800\" /> |\n\n| **2.4** | **Explore server creation** |\n| :--- | :--- |\n| **Narration** | Let's go ahead and create that server. |\n| **Action** &nbsp; 2.4.1 | Click **Create**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-4-1.png\" width=\"800\" /> |\n| **Narration** | We can see a new server appear, but it hasn't “started” yet. Behind the scenes, Kubernetes has received all the instructions it needs to start up – what image to download, how much CPU and memory to provide to it, and other parameters. It’s also been asked to create three replicas of the server and load balance between them. |\n| **Action** &nbsp; 2.4.2 | Explore the server creation dashboard. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-4-2.png\" width=\"800\" /> |\n| **Narration** | Whilst we don’t “need” to know about OpenShift to do this deployment, it might be instructive to take a quick look at what’s happening under the covers; what Kubernetes is doing on our behalf based on our instructions. There is a set of three pods starting up, which house the containers that integrate with our insurer. Those could have been created from a normal Kubernetes command line, using the same custom resource definition file, or (as we’ll see later), by a pipeline that calls the Kubernetes APIs. You can imagine just how easy it is to transition from a manual deployment via this user interface, to creating a scripted, automated deployment. |\n| **Action** &nbsp; 2.4.3 | Open the Openshift Web Console (check here how to open it). On left menu, select **Workloads > Pods** (1). Select **cp4i** project (2). Show the simple-echo-app pods creation (3). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-4-3.png\" width=\"800\" /> |\n| **Narration** | The App Connect Dashboard displays the integration up and running with three replicas. Kubernetes manages high availability implicitly and will ensure there are always three. If one of the replicas fails, Kubernetes will reinstate a new one in its place. |\n| **Action** &nbsp; 2.4.4 | Return to the App Connect Dashboard page. Click **simple-echo-app server**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-4-4.png\" width=\"800\" /> |\n| **Action** &nbsp; 2.4.5 | Open the **HTTPEcho** application. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-4-5.png\" width=\"800\" /> |\n| **Action** &nbsp; 2.4.6 | Open the **Echo** message flow. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-4-6.png\" width=\"800\" /> |\n| **Action** &nbsp; 2.4.7 | Explore the **Echo properties** page. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-4-7.png\" width=\"800\" /> |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>3 - Exploring the Pipeline </summary>\n\n<br/>\n\n| **3.1** | **Create the Pipeline** |\n| :--- | :--- |\n| **Narration** | Next, we will automate deployment of a complex integration solution via a pipeline. This scenario involves various integration capabilities such as integration flows, queues, and managed APIs. As we automate deployment, we’ll add some realistic additional steps to the pipeline such as performing tests in an initial environment before deploying to a second environment.<br/><br/>Now the insurance broker that wants to get quotations from three different companies. As we walk through the insurance broker’s requirements we will see how each of the different integration components comes into play.<br/><br/>- Each of those companies has their own unique way of exposing their quotation system, and so clearly we have build integration flows in App Connect order to help us request those quotes and pull back the answers in a form we can use.<br/><br/>- They want to be able to make the results of this integration available over an API so that their partners can build this into their systems. If they can expose those APIs through an API management system, then it will be easier for partners to onboard themselves to use the APIs. And of course, it will make it easier to potentially monetize those APIs.<br/><br/>- The API needs to be responsive, it needs to respond with as many quotations as possible, as quickly as possible so that the partners get a good service, or they’ll turn to another insurance broker. So they're going to have to make all the requests to all of the different downstream companies at the same time in parallel and get the responses back and merge. Using IBM MQ queues to fan out those three pieces of work is a nice easy way to create that parallelism, and it leads to some possible further optimizations later down the line. <br/><br/>- They want to be able to scale all of these integrations and queuing capabilities independently, but they need to be able to deploy it as a single solution. This suggests they will definitely benefit from deploying in a fine-grained way using containers, so they can update and scale things independently. But it also means that they're going to want to be able to deploy the complete solution in one go in a consistent way through one CICD pipeline. <br/><br/>Even with this relatively simple problem, we've made a case for a fairly complex integration solution involving multiple styles of integration, that requires deployment via a CICD pipeline. <br/><br/>We're going to use the Kubernetes native Tekton pipeline capability that comes as part of OpenShift to deploy our whole insurance integration solution. Here on my machine I forked and cloned a github project with the pipeline. Let's execute a script to create the pipeline in my Openshift environment. |\n| **Action** &nbsp; 3.1.1 | Open a Terminal Window and go to the cp4i-deployment-samples folder cloned in the demo preparation section. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-3-1-1.png\" width=\"800\" /> |\n| **Action** &nbsp; 3.1.2 | Go to the **DrivewayDentDeletion/Operators** folder (**cd DrivewayDentDeletion/Operators/**). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-3-1-2.png\" width=\"800\" /> |\n| **Action** &nbsp; 3.1.3 | Execute the cicd-apply-test-apic-pipeline.sh (**Note**: before you execute the script below, please guarantee that you are already logged in your Openshift environment using OC command line tool, check how to do it here). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-3-1-3.png\" width=\"800\" /> |\n| **Action** &nbsp; 3.1.4 | As part of the result script, you should see the webhook URL. Copied it. You will use it in next step. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-3-1-4.png\" width=\"800\" /> |\n\n| **3.2** | **Explore the Pipeline** |\n| :--- | :--- |\n| **Narration** | Now, let's open the OpenShift Console to check the Pipeline that  we created in the cp4i project. |\n| **Action** &nbsp; 3.2.1 | Back to Openshift Web Console, open **Pipelines > Pipelines**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-3-2-1.png\" width=\"800\" /> |\n| **Action** &nbsp; 3.2.2 | Filter by **cp4i project**. Open **test-apic-pipeline** (3). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-3-2-3.png\" width=\"800\" /> |\n| **Action** &nbsp; 3.2.3 | Open **test-apic-pipeline**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-3-2-3.png\" width=\"800\" /> |\n| **Narration** | As you can see, we specified a pipeline with multiple tasks that builds multiple integration images, a queue manager, and also configures some API exposure. It then deploys them to a test environment, performs some tests, and on successful completion of those, then goes on to deploy them into the main environment. |\n| **Action** &nbsp; 3.2.4 | Explore *test-apic-pipeline**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-3-2-4.png\" width=\"800\" /> |\n\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>4 - Initiating the pipeline from Git </summary>\n\n<br/>\n\n| **4.1** | **Configure the Webhook** |\n| :--- | :--- |\n| **Narration** | To make it a bit more realistic, source code is stored in a source code repository – in this case Git. We're going to set up a web hook on Git which will trigger our Tekton pipeline whenever we make a commit to the code. <br/><br/>Note that it is not just the integration code that could trigger a build. It could also be a change to an MQ Configuration, or a change to the definition of an API.<br/><br/>Furthermore, there are other things that we could, indeed should, store in Git. The custom resource definitions define the environment, so on some level they are “infrastructure as code”. A change to those could trigger a pipeline too. This is the starting point for what is called GitOps, where operators never actually connect directly to the platform, but instead make configuration changes in the implicitly audited source code repository, following the exact same process as developers.<br/><br/>Our insurance broker now has even greater confidence in the consistency of what is in production as the code repository contains absolutely everything required to re-create it. If something happens in production, they need only look at the code repository to understand exactly what changes were recently made, whether infrastructure of integration code. They could also easily build an exact replica of the whole solution to safely diagnose the problem. Perhaps one of the most attractive features of this approach is that they have a complete configuration should they ever need to roll back to a previous version. Imagine how much more comfortable you would be deploying a new business feature, if you knew you could get back to your previous state, rapidly, and with precision.<br/><br/>Here in the Webhook page, we just need to input our webhook URL and define the content type as JSON.  Great, our webhook is ready. It will \"fire\" immediately we create it. |\n| **Action** &nbsp; 4.1.1 | Open your github project forked in Demo preparation part and click **Settings**.<br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-4-1-1.png\" width=\"800\" /> |\n| **Action** &nbsp; 4.1.2 | Click **Webhooks**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-4-1-2.png\" width=\"800\" /> |\n| **Action** &nbsp; 4.1.3 | Click **Add Webhook**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-4-1-3.png\" width=\"800\" /> |\n| **Action** &nbsp; 4.1.4 | In the Payload URL, paste the **webhook address** (1). Change the content type to **application/json** (2). Click **Add webhook** (3).<br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-4-1-4.png\" width=\"800\" /> |\n\n| **4.2** | **Check the Pipeline Run** |\n| :--- | :--- |\n| **Narration** | Let's check our pipeline. Tekton is not only well suited to deploying cloud native solutions, it is itself a cloud native application. It runs on the Kubernetes platform in containers, and directly leverages the rapid deployment and scalability of containers to run pipelines. |\n| **Action** &nbsp; 4.2.1 | Back to OpenShift Web console, open the **Pipelines > Pipeline Runs** page. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-4-2-1.png\" width=\"800\" /> |\n| **Narration** | We can see our pipeline has been started, and we have a high-level view of its progress. |\n| **Action** &nbsp; 4.2.2 | Open the pipeline diagram. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-4-2-2.png\" width=\"800\" /> |\n| **Narration** |You can see there's a nice diagrammatical representation of the pipeline doing its work all the way through to the testing piece in the middle. <br/><br/>The pipeline diagram shows our progress through the build, test, and then deployment to two different environments. We can see it has been initiated, and the builds of the various images are taking place. |\n| **Action** &nbsp; 4.2.3 | Explore the pipeline diagram page. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-4-2-3.png\" width=\"800\" /> |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>5 - Confirming the deployment </summary>\n\n<br/>\n\n| **5.1** | **Check the App Connect deployment** |\n| :--- | :--- |\n| **Narration** | Kubernetes is building a brand new topology unique to this integration solution on the fly, with queues, integrations and managed APIs, and ensuring that they are all able to connect to one another. Furthermore, it is then creating replicas for availability and performance, and implementing the load balancing across them.<br/><br/>For our insurance broker this means that this integration solution is completely independent of all their other integrations. They can re-build the whole thing at will, scale it up or down, refresh the underlying runtimes with new fix paks with no fear that they might disrupt some other part of their business.<br/><br/>Recall that part of the pipeline introduced a set of tests that had to be passed before the solution could be deployed to a second environment. Let’s consider what those tests might be doing.<br/><br/>For our integration brokerage scenario, we can imagine just how complex the testing could be. We might be setting up stubs to make calls against in the background. We might be bringing performance test capabilities to push load into the system. Then we need to tear all of that down before we then go onto the next step of finally evaluating the test results and then deciding whether to push into the target environment. This is one of the real beauties of the elastic nature of container environments. Test environments can be created on demand, and torn down once their purpose has expired.<br/><br/>After only a short while we will see that the integration servers for our solutions are started, but what about the rest of our insurance integration solution. |\n| **Action** &nbsp; 5.1.1 | Open the App Connect Dashboard Server page. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-5-1-1.png\" width=\"800\" /> |\n| **Action** &nbsp; 5.1.2 | Show the **ddd-dev-ace servers**. If you don't see them, refresh the page.\n <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-5-1-2.png\" width=\"800\" /> |\n\n| **5.2** | **Check the queue deployment in MQ explorer** |\n| :--- | :--- |\n| **Narration** | Navigating to the messaging capability we can see that new queue managers have been created with the appropriate queues. These will allow the aggregating integration to initiate all the quotation requests to the insurers in parallel. |\n| **Action** &nbsp; 5.2.1 | Open **Menu** (1) and select **Run** (2) > **Message** (3). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-5-2-1.png\" width=\"800\" /> |\n| **Action** &nbsp; 5.2.2 | Show the **mq-dd-qm-dev** server. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-5-2-2.png\" width=\"800\" /> |\n\n| **5.3** | **Check the API deployment** |\n| :--- | :--- |\n| **Narration** | Navigating to API management Portal, we can see the APIs have been deployed into the appropriate Products.<br/><br/>Potential partners who want to use the insurance brokers new aggregation API will be able to come to the portal and self-subscribe to use it.<br/><br/>It’s worth noting that as we navigated between different underling components we remained in the same user interface, logged in as the same user, and we saw a consistent look and feel to the way each of the components was administered. Furthermore, the navigation and administration capabilities are all governable by a common role based access control mechanism inherited from the underlying OpenShift platform. |\n| **Action** &nbsp; 5.3.1 | Open **Menu** (1) and select **Run** (2) > **API** (3). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-5-3-1.png\" width=\"800\" /> |\n| **Action** &nbsp; 5.3.2 | Select **Common Services User Registry**.<br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-5-3-2.png\" width=\"800\" /> |\n| **Action** &nbsp; 5.3.3 | Check that you are using **main-demo** organization (1). If not, click **Organization** combobox and select the other organization available (2).<br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-5-3-3.png\" width=\"800\" /> |\n| **Action** &nbsp; 5.3.4 | Click **Develop APIs and products**.<br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-5-3-4.png\" width=\"800\" /> |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>6 - Demonstrating Availability and Scalability </summary>\n\n<br/>\n\n| **6.1** | **Scale up the replicas** |\n| :--- | :--- |\n| **Narration** | Let’s imagine that our new insurance brokerage API is much more popular than we expected. We might want to scale up the number of replicas of the integration server to handle the load.<br/><br/>For simplicity in this example, we will edit the custom resource definition directly through the UI so we can quickly see the scaling taking place. However, note that in a real scenario we would probably now do this in the source code repository and let the GitOps pipeline bring the change into production.<br/><br/>We will change the number of replicas, and say move it from three to four. That provides a \"future state” requirements and the underlying Kubernetes infrastructure will now do all the operational work needed to get it to that new state. No infrastructure expansion project, no manual infrastructure changes, just a change of a number in a file. |\n| **Action** &nbsp; 6.1.1 | Open **Menu** (1) and select **Run** (2) > **Integration** (3). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-6-1-1.png\" width=\"800\" /> |\n| **Action** &nbsp; 6.1.2 | Open **Servers**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-6-1-2.png\" width=\"800\" /> |\n| **Action** &nbsp; 6.1.3 | Click the **ddd-dev-ace-api** (1) context menu and select **Edit** (2).  <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-6-1-3.png\" width=\"800\" /> |\n| **Action** &nbsp; 6.1.4 | Type **4** in the **Replicas** field (1). Click **Update** (2). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-6-1-4.png\" width=\"800\" /> |\n| **Action** &nbsp; 6.1.5 | Ono the OpenShift Web Console page, open the **Workloads > Pods** page (1), filter by **cp4i** project (2) and show the four pods of the ddd-dev-ace-api (3). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-6-1-5.png\" width=\"800\" /> |\n\n| **6.2** | **Show high availability** |\n| :--- | :--- |\n| **Narration** | Finally, we're going to emulate a failure by deleting one of the pods that looks after the integration containers and watching it come back up again. We will see Kubernetes and OpenShift doing its job, making sure that that state that we've requested matches with what is actually deployed and running. That promise of operational reliability is not something you have to build in, it is just fundamental to the way that Kubernetes works. | \n| **Action** &nbsp; 6.2.1 | On Openshift Web Console Pods page, click the context menu of a ddd-dev-ace-api pod and select **Delete Pod** (2).<br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-6-2-1.png\" width=\"800\" /> |\n| **Action** &nbsp; 6.2.2 | Show the recreation of the pod.<br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-6-2-2.png\" width=\"800\" /> |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>Summary </summary>\n\n<br/>\n\nEverything you've seen here is just performing normal Kubernetes commands under the covers. These things could be done against a command line, and they could be done over the Kubernetes APIs, and so we have this consistent and well understood mechanism for operating the entire system, across all the different capabilities within the Cloud Pak for Integration.  \n\n<br/>\n\nSo, we can see that our insurance broker can focus on defining and implementing new value add features to their integrations and let Kubernetes take over much of the day to day infrastructural and operational tasks. Furthermore, it does this in a uniform and consistent way this will be identical operationally to all other technologies in the enterprise that are running on containers, reducing training requirement, and simplifying skill sharing across teams. \n\n<br/>\n\nOnce your human operators know how to handle Kubernetes and OpenShift, they’re nine tenths of the way to knowing how to handle Cloud Pak for Integration. \n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n","type":"Mdx","contentDigest":"e677cf5f6c78fb065e437b6ad532e7a9","owner":"gatsby-plugin-mdx","counter":2200},"frontmatter":{"title":"Automating deployment of multi-style integrations <br/> 300-level live demo 300-level live demo","description":"Dent deletion 300-level live demo","tabs":["Demo preparation","Demo script"]},"exports":{},"rawBody":"---\ntitle: Automating deployment of multi-style integrations <br/> 300-level live demo\n  300-level live demo\ndescription: Dent deletion 300-level live demo\ntabs: [ 'Demo preparation', 'Demo script']\n---\n\nexport const Title = () => (\n  <span>\n      Automating deployment of multi-style integrations <br/> 300-level live demo\n  </span> );\n\n![PENDING Dent Deletion banner](./images/Predictive-Decisioning-300-Script-GitHub-banner-12-15-21.jpg)\n\n<span id=\"top\"></span>\n\n<details>\n\n<summary>Introduction</summary>\n\n<br/>\n\nWe’re going to demonstrate how to automate deployment of a complex integration solution using a pipeline. This enables faster, more frequent delivery of changes into production and improves deployment confidence. We’re also going to show how container-based platforms enable operational consistency and automation (across those capabilities), simplifying administration of an environment.\n\n<br/>\n\nOur example scenario features an insurance broker gathering insurance quotes from multiple companies and providing an aggregated list to customers via an API. This involves multiple integration styles, including application integration, API management, and messaging. Our goal is automated deployment and operations for the process.\n\n<br/>\n\nLet's begin with a simple deployment of a single integration: an integration that retrieves a quote from an insurer. We’re going to deploy this integration using cloud native techniques on containers. You will see that you don’t need to know anything about containers, Kubernetes or OpenShift (Red Hat’s Kubernetes platform) to  deploy a simple integration.  \n\n<br/>\n\nLater, we will explore a more complex solution, with multiple integration abilities that we want to deploy together - including integration flows, queues, and managed APIs.\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>1 - Accessing the Environment</summary>\n\n<br/>\n\n| **1.1** | **Log into Cloud Pak for Integration** |\n| :--- | :--- |\n| **Narration** | Let’s see IBM Cloud Pak for Integration in action. |\n| **Action** &nbsp; 1.1.1 | Open Cloud Pak for Integration and click **IBM provided credentials (admin only)**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-1-1-1.png\" width=\"800\" /> |\n| **Narration** | Let's log in to a cloud version on IBM Cloud. |\n| **Action** &nbsp; 1.1.2 | **Log in** with your admin **username** and **password**.  <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-1-1-2.png\" width=\"800\" /> |\n\n| **1.2** | **View the Cloud Pak for Integration home screen** |\n| :--- | :--- |\n| **Narration** | This is the IBM Cloud Pak for Integration home screen, which shows all the capabilities of the pak in one place. Specialized integration capabilities for API management, application integration, messaging, and more, are built on top of powerful automation services. Let’s see the integration capabilities available. |\n| **Action** &nbsp; 1.2.1 | From the **Home Page**, click **Integration Capabilities** (if necessary close the tour dialog). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-1-2-1.png\" width=\"800\" /> |\n\n| **1.3** | **View the Cloud Pak for Integration home screen (same as step 1.2)** |\n| :--- | :--- |\n| **Narration** | You are able to access all the integration capabilities your team needs through a single interface, including API management, application integration, enterprise messaging, events, and high-speed transfer. To automate customer interactions in this demo, we will use App Connect for application integration, API Connect for API management, and the Message Queue for Enterprise Messaging.<br/><br/>Let’s open our App Connect Dashboard. |\n| **Action** &nbsp; 1.3.1 | On the **Integration Capabilities** page, click the **Integration dashboard** (ace-dashboard-demo). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-1-3-1.png\" width=\"800\" /><br/> |\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>2 - Deploying your Integration </summary>\n\n<br/>\n\n| **2.1** | **Create an Integration Server** |\n| :--- | :--- |\n| **Narration** | We’ll create an integration server our new integration deplyment. Each integration server is deployed in a separate container. |\n| **Action** &nbsp; 2.1.1 | Click **Create a server**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-1-1.png\" width=\"800\" /> |\n\n| **2.2** | **Import the BAR file** |\n| :--- | :--- |\n| **Narration** | In this example, we're going to deploy an integration that we've already created in the App Connect toolkit. We can simply drag and drop it into the console straight from the file system. If we’ve loaded the bar file before, it will be available from an internal asset repository. |\n| **Action** &nbsp; 2.2.1 | Select **Quick start toolkit integration** (1). Click **Next** (2). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-2-1.png\" width=\"800\" /> |\n| **Action** &nbsp; 2.2.2 | Upload the **HTTPEchoApp.bar** BAR file (1) that you downloaded during demo preparation. Click **Next** (2). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-2-2.png\" width=\"800\" /> |\n\n| **2.3** | **Configure your integration server** |\n| :--- | :--- |\n| **Narration** | We are then asked if we would like to apply any specific configuration information to this particular deployment, such as user credentials or certificates that are required to integrate to a particular backend system. In our case, this might include credentials to the downstream insurance quotation engine. These are held within Kubernetes in standard mechanisms known as ConfigMaps and Secrets, with visibilility limited to those with correct permissions to view, upload and change credentials. <br/><br/>(Having such a clear separation of things that change per environment (e.g. credentials) and things that don’t (e.g. integration code) improves our deployment confidence, and enables governed usage of sensitive information. We reduce errors due to “drift” between the configuration of one environment and another, and reduce the chance of credentials getting into the wrong hands. This is a good practice generally, and container technology makes this separation of concerns simpler and clearer. <br/><br/>This is particularly important in integration scenarios - such as our insurance brokerage example - which have credentials enabling access to multiple third-party insurers. Those systems may provide access to sensitive personal data. As an exmaple, the insurers may be charging the broker for access to their API, or conversely, providing commission on quotes that are followed up. Misuse of the credentials could have all manner of undesired effects.) |\n| **Action** &nbsp; 2.3.1 | Click **Next**.<br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-3-1.png\" width=\"800\" /> |\n| **Narration** | We are then asked to fill in a form, a simple way of populating a file that will be used during deployment. The file has the details about how your integration will be deployed and operated. We can decide things such as the version of the runtime that we want the integration to run against, and that is specific to this particular integration. Another integration might be running against a different version that it was tested against. <br/><br/>This is also where we decide how many replicas of the integration container we want. In a traditional installation, all integrations would inherit the same characteristics of the high availability pair of the centralised servers. In our scenario, our customers might be particularly sensitive to outages, so we might decide to minimize the availabiltiy impact of any runtime failures by increasing the number of container replicas to \"3\", or \"5, or \"7\".<br/><br/>We can also change the amount of memory assigned to the container if we knew the data model for the insurance quotation was particularly large and therefore memory intensive. |\n| **Action** &nbsp; 2.3.2 | Name the integration server **simple-echo-app** (1). Change the **Replicas** number to **3** (2). Open the **YAML editor** (3).<br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-3-2.png\" width=\"800\" /> |\n| **Narration** | This form is just a graphical way of editing the yaml formatted deployment properties file. You can see it's doing nothing more than updating that text file, and we can see our number of replicas has been set to \"3\". This file is known as the “custom resource definition” for the integration, but again, you don’t actually need to know that to deploy from here. This is the file that you would use to instantiate this integration using command line tools, or by calling one of the OpenShift APIs, or from a pipeline - as we will do later in this demonstration. |\n| **Action** &nbsp; 2.3.3 | Open the **Common settings** tab. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-3-3.png\" width=\"800\" /> |\n\n| **2.4** | **Explore server creation** |\n| :--- | :--- |\n| **Narration** | Let's go ahead and create that server. |\n| **Action** &nbsp; 2.4.1 | Click **Create**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-4-1.png\" width=\"800\" /> |\n| **Narration** | We can see a new server appear, but it hasn't “started” yet. Behind the scenes, Kubernetes has received all the instructions it needs to start up – what image to download, how much CPU and memory to provide to it, and other parameters. It’s also been asked to create three replicas of the server and load balance between them. |\n| **Action** &nbsp; 2.4.2 | Explore the server creation dashboard. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-4-2.png\" width=\"800\" /> |\n| **Narration** | Whilst we don’t “need” to know about OpenShift to do this deployment, it might be instructive to take a quick look at what’s happening under the covers; what Kubernetes is doing on our behalf based on our instructions. There is a set of three pods starting up, which house the containers that integrate with our insurer. Those could have been created from a normal Kubernetes command line, using the same custom resource definition file, or (as we’ll see later), by a pipeline that calls the Kubernetes APIs. You can imagine just how easy it is to transition from a manual deployment via this user interface, to creating a scripted, automated deployment. |\n| **Action** &nbsp; 2.4.3 | Open the Openshift Web Console (check here how to open it). On left menu, select **Workloads > Pods** (1). Select **cp4i** project (2). Show the simple-echo-app pods creation (3). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-4-3.png\" width=\"800\" /> |\n| **Narration** | The App Connect Dashboard displays the integration up and running with three replicas. Kubernetes manages high availability implicitly and will ensure there are always three. If one of the replicas fails, Kubernetes will reinstate a new one in its place. |\n| **Action** &nbsp; 2.4.4 | Return to the App Connect Dashboard page. Click **simple-echo-app server**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-4-4.png\" width=\"800\" /> |\n| **Action** &nbsp; 2.4.5 | Open the **HTTPEcho** application. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-4-5.png\" width=\"800\" /> |\n| **Action** &nbsp; 2.4.6 | Open the **Echo** message flow. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-4-6.png\" width=\"800\" /> |\n| **Action** &nbsp; 2.4.7 | Explore the **Echo properties** page. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-2-4-7.png\" width=\"800\" /> |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>3 - Exploring the Pipeline </summary>\n\n<br/>\n\n| **3.1** | **Create the Pipeline** |\n| :--- | :--- |\n| **Narration** | Next, we will automate deployment of a complex integration solution via a pipeline. This scenario involves various integration capabilities such as integration flows, queues, and managed APIs. As we automate deployment, we’ll add some realistic additional steps to the pipeline such as performing tests in an initial environment before deploying to a second environment.<br/><br/>Now the insurance broker that wants to get quotations from three different companies. As we walk through the insurance broker’s requirements we will see how each of the different integration components comes into play.<br/><br/>- Each of those companies has their own unique way of exposing their quotation system, and so clearly we have build integration flows in App Connect order to help us request those quotes and pull back the answers in a form we can use.<br/><br/>- They want to be able to make the results of this integration available over an API so that their partners can build this into their systems. If they can expose those APIs through an API management system, then it will be easier for partners to onboard themselves to use the APIs. And of course, it will make it easier to potentially monetize those APIs.<br/><br/>- The API needs to be responsive, it needs to respond with as many quotations as possible, as quickly as possible so that the partners get a good service, or they’ll turn to another insurance broker. So they're going to have to make all the requests to all of the different downstream companies at the same time in parallel and get the responses back and merge. Using IBM MQ queues to fan out those three pieces of work is a nice easy way to create that parallelism, and it leads to some possible further optimizations later down the line. <br/><br/>- They want to be able to scale all of these integrations and queuing capabilities independently, but they need to be able to deploy it as a single solution. This suggests they will definitely benefit from deploying in a fine-grained way using containers, so they can update and scale things independently. But it also means that they're going to want to be able to deploy the complete solution in one go in a consistent way through one CICD pipeline. <br/><br/>Even with this relatively simple problem, we've made a case for a fairly complex integration solution involving multiple styles of integration, that requires deployment via a CICD pipeline. <br/><br/>We're going to use the Kubernetes native Tekton pipeline capability that comes as part of OpenShift to deploy our whole insurance integration solution. Here on my machine I forked and cloned a github project with the pipeline. Let's execute a script to create the pipeline in my Openshift environment. |\n| **Action** &nbsp; 3.1.1 | Open a Terminal Window and go to the cp4i-deployment-samples folder cloned in the demo preparation section. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-3-1-1.png\" width=\"800\" /> |\n| **Action** &nbsp; 3.1.2 | Go to the **DrivewayDentDeletion/Operators** folder (**cd DrivewayDentDeletion/Operators/**). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-3-1-2.png\" width=\"800\" /> |\n| **Action** &nbsp; 3.1.3 | Execute the cicd-apply-test-apic-pipeline.sh (**Note**: before you execute the script below, please guarantee that you are already logged in your Openshift environment using OC command line tool, check how to do it here). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-3-1-3.png\" width=\"800\" /> |\n| **Action** &nbsp; 3.1.4 | As part of the result script, you should see the webhook URL. Copied it. You will use it in next step. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-3-1-4.png\" width=\"800\" /> |\n\n| **3.2** | **Explore the Pipeline** |\n| :--- | :--- |\n| **Narration** | Now, let's open the OpenShift Console to check the Pipeline that  we created in the cp4i project. |\n| **Action** &nbsp; 3.2.1 | Back to Openshift Web Console, open **Pipelines > Pipelines**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-3-2-1.png\" width=\"800\" /> |\n| **Action** &nbsp; 3.2.2 | Filter by **cp4i project**. Open **test-apic-pipeline** (3). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-3-2-3.png\" width=\"800\" /> |\n| **Action** &nbsp; 3.2.3 | Open **test-apic-pipeline**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-3-2-3.png\" width=\"800\" /> |\n| **Narration** | As you can see, we specified a pipeline with multiple tasks that builds multiple integration images, a queue manager, and also configures some API exposure. It then deploys them to a test environment, performs some tests, and on successful completion of those, then goes on to deploy them into the main environment. |\n| **Action** &nbsp; 3.2.4 | Explore *test-apic-pipeline**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-3-2-4.png\" width=\"800\" /> |\n\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>4 - Initiating the pipeline from Git </summary>\n\n<br/>\n\n| **4.1** | **Configure the Webhook** |\n| :--- | :--- |\n| **Narration** | To make it a bit more realistic, source code is stored in a source code repository – in this case Git. We're going to set up a web hook on Git which will trigger our Tekton pipeline whenever we make a commit to the code. <br/><br/>Note that it is not just the integration code that could trigger a build. It could also be a change to an MQ Configuration, or a change to the definition of an API.<br/><br/>Furthermore, there are other things that we could, indeed should, store in Git. The custom resource definitions define the environment, so on some level they are “infrastructure as code”. A change to those could trigger a pipeline too. This is the starting point for what is called GitOps, where operators never actually connect directly to the platform, but instead make configuration changes in the implicitly audited source code repository, following the exact same process as developers.<br/><br/>Our insurance broker now has even greater confidence in the consistency of what is in production as the code repository contains absolutely everything required to re-create it. If something happens in production, they need only look at the code repository to understand exactly what changes were recently made, whether infrastructure of integration code. They could also easily build an exact replica of the whole solution to safely diagnose the problem. Perhaps one of the most attractive features of this approach is that they have a complete configuration should they ever need to roll back to a previous version. Imagine how much more comfortable you would be deploying a new business feature, if you knew you could get back to your previous state, rapidly, and with precision.<br/><br/>Here in the Webhook page, we just need to input our webhook URL and define the content type as JSON.  Great, our webhook is ready. It will \"fire\" immediately we create it. |\n| **Action** &nbsp; 4.1.1 | Open your github project forked in Demo preparation part and click **Settings**.<br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-4-1-1.png\" width=\"800\" /> |\n| **Action** &nbsp; 4.1.2 | Click **Webhooks**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-4-1-2.png\" width=\"800\" /> |\n| **Action** &nbsp; 4.1.3 | Click **Add Webhook**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-4-1-3.png\" width=\"800\" /> |\n| **Action** &nbsp; 4.1.4 | In the Payload URL, paste the **webhook address** (1). Change the content type to **application/json** (2). Click **Add webhook** (3).<br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-4-1-4.png\" width=\"800\" /> |\n\n| **4.2** | **Check the Pipeline Run** |\n| :--- | :--- |\n| **Narration** | Let's check our pipeline. Tekton is not only well suited to deploying cloud native solutions, it is itself a cloud native application. It runs on the Kubernetes platform in containers, and directly leverages the rapid deployment and scalability of containers to run pipelines. |\n| **Action** &nbsp; 4.2.1 | Back to OpenShift Web console, open the **Pipelines > Pipeline Runs** page. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-4-2-1.png\" width=\"800\" /> |\n| **Narration** | We can see our pipeline has been started, and we have a high-level view of its progress. |\n| **Action** &nbsp; 4.2.2 | Open the pipeline diagram. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-4-2-2.png\" width=\"800\" /> |\n| **Narration** |You can see there's a nice diagrammatical representation of the pipeline doing its work all the way through to the testing piece in the middle. <br/><br/>The pipeline diagram shows our progress through the build, test, and then deployment to two different environments. We can see it has been initiated, and the builds of the various images are taking place. |\n| **Action** &nbsp; 4.2.3 | Explore the pipeline diagram page. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-4-2-3.png\" width=\"800\" /> |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>5 - Confirming the deployment </summary>\n\n<br/>\n\n| **5.1** | **Check the App Connect deployment** |\n| :--- | :--- |\n| **Narration** | Kubernetes is building a brand new topology unique to this integration solution on the fly, with queues, integrations and managed APIs, and ensuring that they are all able to connect to one another. Furthermore, it is then creating replicas for availability and performance, and implementing the load balancing across them.<br/><br/>For our insurance broker this means that this integration solution is completely independent of all their other integrations. They can re-build the whole thing at will, scale it up or down, refresh the underlying runtimes with new fix paks with no fear that they might disrupt some other part of their business.<br/><br/>Recall that part of the pipeline introduced a set of tests that had to be passed before the solution could be deployed to a second environment. Let’s consider what those tests might be doing.<br/><br/>For our integration brokerage scenario, we can imagine just how complex the testing could be. We might be setting up stubs to make calls against in the background. We might be bringing performance test capabilities to push load into the system. Then we need to tear all of that down before we then go onto the next step of finally evaluating the test results and then deciding whether to push into the target environment. This is one of the real beauties of the elastic nature of container environments. Test environments can be created on demand, and torn down once their purpose has expired.<br/><br/>After only a short while we will see that the integration servers for our solutions are started, but what about the rest of our insurance integration solution. |\n| **Action** &nbsp; 5.1.1 | Open the App Connect Dashboard Server page. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-5-1-1.png\" width=\"800\" /> |\n| **Action** &nbsp; 5.1.2 | Show the **ddd-dev-ace servers**. If you don't see them, refresh the page.\n <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-5-1-2.png\" width=\"800\" /> |\n\n| **5.2** | **Check the queue deployment in MQ explorer** |\n| :--- | :--- |\n| **Narration** | Navigating to the messaging capability we can see that new queue managers have been created with the appropriate queues. These will allow the aggregating integration to initiate all the quotation requests to the insurers in parallel. |\n| **Action** &nbsp; 5.2.1 | Open **Menu** (1) and select **Run** (2) > **Message** (3). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-5-2-1.png\" width=\"800\" /> |\n| **Action** &nbsp; 5.2.2 | Show the **mq-dd-qm-dev** server. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-5-2-2.png\" width=\"800\" /> |\n\n| **5.3** | **Check the API deployment** |\n| :--- | :--- |\n| **Narration** | Navigating to API management Portal, we can see the APIs have been deployed into the appropriate Products.<br/><br/>Potential partners who want to use the insurance brokers new aggregation API will be able to come to the portal and self-subscribe to use it.<br/><br/>It’s worth noting that as we navigated between different underling components we remained in the same user interface, logged in as the same user, and we saw a consistent look and feel to the way each of the components was administered. Furthermore, the navigation and administration capabilities are all governable by a common role based access control mechanism inherited from the underlying OpenShift platform. |\n| **Action** &nbsp; 5.3.1 | Open **Menu** (1) and select **Run** (2) > **API** (3). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-5-3-1.png\" width=\"800\" /> |\n| **Action** &nbsp; 5.3.2 | Select **Common Services User Registry**.<br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-5-3-2.png\" width=\"800\" /> |\n| **Action** &nbsp; 5.3.3 | Check that you are using **main-demo** organization (1). If not, click **Organization** combobox and select the other organization available (2).<br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-5-3-3.png\" width=\"800\" /> |\n| **Action** &nbsp; 5.3.4 | Click **Develop APIs and products**.<br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-5-3-4.png\" width=\"800\" /> |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>6 - Demonstrating Availability and Scalability </summary>\n\n<br/>\n\n| **6.1** | **Scale up the replicas** |\n| :--- | :--- |\n| **Narration** | Let’s imagine that our new insurance brokerage API is much more popular than we expected. We might want to scale up the number of replicas of the integration server to handle the load.<br/><br/>For simplicity in this example, we will edit the custom resource definition directly through the UI so we can quickly see the scaling taking place. However, note that in a real scenario we would probably now do this in the source code repository and let the GitOps pipeline bring the change into production.<br/><br/>We will change the number of replicas, and say move it from three to four. That provides a \"future state” requirements and the underlying Kubernetes infrastructure will now do all the operational work needed to get it to that new state. No infrastructure expansion project, no manual infrastructure changes, just a change of a number in a file. |\n| **Action** &nbsp; 6.1.1 | Open **Menu** (1) and select **Run** (2) > **Integration** (3). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-6-1-1.png\" width=\"800\" /> |\n| **Action** &nbsp; 6.1.2 | Open **Servers**. <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-6-1-2.png\" width=\"800\" /> |\n| **Action** &nbsp; 6.1.3 | Click the **ddd-dev-ace-api** (1) context menu and select **Edit** (2).  <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-6-1-3.png\" width=\"800\" /> |\n| **Action** &nbsp; 6.1.4 | Type **4** in the **Replicas** field (1). Click **Update** (2). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-6-1-4.png\" width=\"800\" /> |\n| **Action** &nbsp; 6.1.5 | Ono the OpenShift Web Console page, open the **Workloads > Pods** page (1), filter by **cp4i** project (2) and show the four pods of the ddd-dev-ace-api (3). <br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-6-1-5.png\" width=\"800\" /> |\n\n| **6.2** | **Show high availability** |\n| :--- | :--- |\n| **Narration** | Finally, we're going to emulate a failure by deleting one of the pods that looks after the integration containers and watching it come back up again. We will see Kubernetes and OpenShift doing its job, making sure that that state that we've requested matches with what is actually deployed and running. That promise of operational reliability is not something you have to build in, it is just fundamental to the way that Kubernetes works. | \n| **Action** &nbsp; 6.2.1 | On Openshift Web Console Pods page, click the context menu of a ddd-dev-ace-api pod and select **Delete Pod** (2).<br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-6-2-1.png\" width=\"800\" /> |\n| **Action** &nbsp; 6.2.2 | Show the recreation of the pod.<br/><br/><img src=\"https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-dent-deletion/images/dent-deletion-6-2-2.png\" width=\"800\" /> |\n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n\n<details>\n\n<summary>Summary </summary>\n\n<br/>\n\nEverything you've seen here is just performing normal Kubernetes commands under the covers. These things could be done against a command line, and they could be done over the Kubernetes APIs, and so we have this consistent and well understood mechanism for operating the entire system, across all the different capabilities within the Cloud Pak for Integration.  \n\n<br/>\n\nSo, we can see that our insurance broker can focus on defining and implementing new value add features to their integrations and let Kubernetes take over much of the day to day infrastructural and operational tasks. Furthermore, it does this in a uniform and consistent way this will be identical operationally to all other technologies in the enterprise that are running on containers, reducing training requirement, and simplifying skill sharing across teams. \n\n<br/>\n\nOnce your human operators know how to handle Kubernetes and OpenShift, they’re nine tenths of the way to knowing how to handle Cloud Pak for Integration. \n\n<br/>\n\n**[Go to top](#top)**\n\n</details>\n","fileAbsolutePath":"/home/runner/work/platinum-demos/platinum-demos/src/pages/300-integration-dent-deletion/demo-script.mdx"}}},"staticQueryHashes":["1364590287","137577622","137577622","2102389209","2102389209","2456312558","2746626797","2746626797","3018647132","3018647132","3037994772","3037994772","768070550"]}