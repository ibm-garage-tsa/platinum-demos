---
title: Scalable and resilient cloud native integration
  300-level live demo
description: Scalable and resilient cloud native integration
tabs: [ 'Demo preparation', 'Demo script']
---

export const Title = () => (
  <span>
    <h1 style="font-size:60px;">Scalable and resilient cloud native integration
 <br/> 300-level live demo</h1>
  </span> );

![banner](BANNER IMAGE COMING)
<span id="place1"></span>

<details>

<summary>Introduction</summary>

<br/>

Today we will see how Focus Bank maintain and enhance their cloud native integration. They recently implemented a new mobile application that requires access to the core banking system. This application and the integration it relies on were built using cloud native principles allowing it to scale elastically and to be resilient to failure.

<br/>

We’ll see the behavior of the integration during a maintenance window where IBM MQ and IBM App Connect are restarted. Then Focus Bank will release an upgrade to the application, increasing the product versions of IBM MQ, and deploying new features for the application. Finally, Focus Bank will scale out the application to handle an expected increase in load, and we’ll see how this is transparent to the application.

<br/>

Let’s get started!

<br/>

(Demo intro slides <a href="LINK COMING" target="_blank" rel="noreferrer">here</a>)

<br/>

(Printer-ready PDF of demo script <a href="https://ibm.box.com/s/6siayii6qbus0v8m5pi293ntgphzbwh7" target="_blank" rel="noreferrer">here</a>)

<br/>

</details>

<details>

<summary>1 - Ensuring continuous availability during a change to the integration solution</summary>

<br/>

| **1.1** | **Understand the application integration flow** |
| :--- | :--- |
| **Narration** | Focus Bank have implemented a mobile application for their customers, allowing them to complete their day-to-day banking needs. This requires access to the core banking system, and Focus Bank decided to implement a resilient and scalable cloud native integration. <br/><br/>The performance dashboard shows the end users' mobile phone requesting operations against the core banking system. These mobile requests are distributed across three IBM App Connect runtimes, but this can be reduced and increased based on the load. The bank uses IBM MQ for internal connectivity. The App Connect runtimes transform the HTTP mobile app requests into MQ messages for the core banking system. Like App Connect, MQ also has multiple runtimes with the ability to scale the number of instances based on the demand. <br/><br/>The architecture has been deliberately created to allow calls to pass through any of the MQ instances to assure a loosely coupled topology. For complete resilience in the case of a failure, a different MQ instance can even be used for the request and response of a single interaction. |
| **Action** &nbsp; 1.1.1 | Show the application dashboard, and walk through as outlined in the narration above. <br/><br/> <img src="https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-managing-events-and-apis-from-unified-environment/images/1-1-1-applications-dashboard.png" width="800" /> |

| **1.2** | **Failover App Connect** |
| :--- | :--- |
| **Narration** | The infrastructure team are upgrading the worker nodes running the application. This causes containers to be restarted as the upgrade ripples through the environment. Although it should not impact end users as each component of the solution is resilient to failure, the integration team want to watch the real-time traffic as this is the first change since the application went live. <br/><br/>The App Connect and MQ graphs will change as the containers are deleted and re-started on another machine. The integration team expect to see the mobile app throughput graph remain constant throughout the upgrade. <br/><br/>We will simulate the worker node upgrade by deleting the running App Connect containers within the Red Hat OpenShift console.<br/><br/>You will notice the throughput of one of the App Connect runtimes will drop to zero and then automatically recover. |
| **Action** &nbsp; 1.2.1 | Copy the identifier for the middle App Connect runtime. <br/><br/> <img src="https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-managing-events-and-apis-from-unified-environment/images/1-2-1-copy-identifier.png" width="800" /> |
| **Action** &nbsp; 1.2.2 | Change tabs to the Red Hat OpenShift console, navigate to **Workload -> Pods** (1), paste the identifier into the filter (2) and press enter. <br/><br/> <img src="https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-managing-events-and-apis-from-unified-environment/images/1-2-2-paste-identifier.png" width="800" /> |
| **Action** &nbsp; 1.2.3 | Click the overflow menu (1) and select **Delete Pod** (2). <br/><br/> <img src="https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-managing-events-and-apis-from-unified-environment/images/1-2-3-delete-pod.png" width="800" /> |
| **Action** &nbsp; 1.2.4 | Confirm the deletion by clicking **Delete**.<br/><br/> <img src="https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-managing-events-and-apis-from-unified-environment/images/1-2-4-confirm-delete.png" width="800" /> |
| **Narration** | The integration team notice the throughput dropping to zero on the App Connect runtime in the middle of the dashboard. The remaining two instances immediately take over the load. This is expected as the team designed the system with the assumption that the remaining two runtimes would be able to handle the load. |
| **Action** &nbsp; 1.2.5 | Return immediately to the application dashboard and show the throughput of the middle runtime has dropped to zero, while the other two have increased. <br/><br/> <img src="https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-managing-events-and-apis-from-unified-environment/images/1-2-5-application-dashboard.png" width="800" /> |
| **Narration** | After a minute they see the identifier for the middle runtime change to the identity of the new container. They see traffic immediately being distributed across all three instances.<br/><br/>They are happy to see the mobile app traffic continued to be processed during the failover. |
| **Action** &nbsp; 1.2.6 | Wait for the middle runtimes name to change, and highlight that the traffic is automatically re-balanced.<InlineNotification>It's common that the MQ traffic may be out of balance for a couple of seconds. This is while the connections from the new App Connect runtime are automatically rebalanced by MQ. </InlineNotification><br/><br/> <img src="https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-managing-events-and-apis-from-unified-environment/images/1-2-6-traffic-rebalanced.png" width="800" /> |
| **Action** &nbsp; 1.2.7 | Highlight how the traffic is now fully re-balanced across all runtimes and the mobile app throughput never changed.<br/><br/> <img src="https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-managing-events-and-apis-from-unified-environment/images/1-2-7-throughput-unchanged.png" width="800" /> |

| **1.3** | **Failover MQ** |
| :--- | :--- |
| **Narration** | Each MQ Queue Manager has been deployed using the Native HA feature. This means three containers, each on separate worker nodes, coordinate to provide a single logical Queue Manager with very high availability (HA). Each container has its own data store, but only one of these containers is the leader, with the remaining two as followers. The leader ensures the followers also receive a copy of all the messages. If the leader fails, a new leader is promoted within seconds. br/><br/> To enable scaling, traffic from applications is spread evenly across queue managers grouped into a “uniform cluster”. In our case there are two such queue managers in the cluster, but more could be added as needed providing linear horizontal scalability.br/><br/> Let’s simulate the worker node upgrade by deleting the running MQ container within the Red Hat OpenShift console. |
| **Action** &nbsp; 1.3.1 | Change tabs to the Red Hat OpenShift console. Navigate to **Workload -> Pods** (1), type **ucqm2-** into the filter (2) and press enter.<br/><br/> <img src="https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-managing-events-and-apis-from-unified-environment/images/1-3-1-press-enter.png" width="800" /> |
| **Action** &nbsp; 1.3.2 | Select the overflow menu (1) for the currently active container of the Native HA set (the one with **Ready** state of **1/1**) (1), and select **Delete Pod** (3).<br/><br/> <img src="https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-managing-events-and-apis-from-unified-environment/images/1-3-2-delete-pod.png" width="800" /> |
| **Action** &nbsp; 1.3.3 | Confirm the deletion by clicking **Delete**.<br/><br/> <img src="https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-managing-events-and-apis-from-unified-environment/images/1-3-3-click-delete.png" width="800" /> |
| **Narration** | As the team watch they see the traffic briefly drop for queue manager ucqm2, with ucqm1 taking over the load, before the load returning to ucqm2 once it has recovered as expected. <br/><br/>They are happy to see the mobile app traffic continued to be processed at the same rate despite the failover. |
| **Action** &nbsp; 1.3.4 | Immediately return to the application dashboard, to see the decline in throughput for ucqm2, and the increase for ucqm1 (1). You are unlikely to see the throughput decline to zero as the MQ recovery is so rapid. Again, highlight that the mobile app traffic remains constant, with no message loss (2).<br/><br/> <img src="https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-managing-events-and-apis-from-unified-environment/images/1-3-4-dashboard-decline.png" width="800" /> |

<br/>

**[Go to top](#place1)**

</details>

<details>

<summary>2 - Releasing a new version of the integration solution</summary>

<br/>

| :--- | :--- |
| **Narration** | The mobile app team has requested a new feature where a push notification occurs for each transaction, confirming the transaction has been successful. This involves a copy of the response message from the core banking system being sent to the notification service. The team uses a feature in IBM MQ called streaming queues. They write a line of MQ configuration to stream messages to the notification queue. <br/><br/>A new version of IBM MQ was recently released and we are taking the opportunity to move to this release in the same time we update the mobile functionality. <br/><br/>The deployment of the application is automated using a pipeline technology called Tekton which is built into OpenShift, with all the deployment artefacts within GitHub as declarative configuration. <br/><br/>The team historically implemented MQ configuration changes monthly, and MQ product version upgrades every 2 years. Implementing these changes through the pipeline allows them to be made in a repeatable way. The cloud native resilient design means changes can be applied without affecting the live traffic. This has transformed how the team operate and the speed they can work at. |
| **2.1** | **Trigger a change from a source repository** |
| :--- | :--- |
| **Narration** | The team has a pipeline to deploy the solution this will be used to complete the updates. They use a ripple deployment to assure one queue manager is updated at a time, protecting the overall availability. <br/><br/>There are multiple ways the pipeline can be triggered. For instance, a change to the code within a GitHub repository can cause an event that triggers the pipeline. In our case we will simulate this triggering by clicking on the Deploy button within the application dashboard.<br/><br/>This pulls a different GitHub branch which contains the updated artifacts and applies them to the environment. |
| **Action** &nbsp; 2.1.1 | In a new browser tab, open the <a href="https://github.com/callumpjackson/cloud-native-integration/blob/notification/mq/uniformcluster/deploy/uniformclusterQMConfig.yaml_template" target="_blank" rel="noreferrer">code repository</a>. Show the **alter** line (1) that configures messages to be streamed from the existing response queue. <br/><br/> <img src="https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-managing-events-and-apis-from-unified-environment/images/2-1-1-alter-line.png" width="800" /> |
| **Action** &nbsp; 2.1.2 | Show the updated version number for IBM MQ within <a href="https://github.com/callumpjackson/cloud-native-integration/blob/notification/mq/uniformcluster/deploy/uniformclusterQM1.yaml_template" target="_blank" rel="noreferrer">GitHub</a>. <br/><br/> <img src="https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-managing-events-and-apis-from-unified-environment/images/2-1-2-updated-version.png" width="800" /> |
| **Action** &nbsp; 2.1.3 | Return to the application dashboard, scroll down, and click the **Deploy** button. <br/><br/> <img src="https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-managing-events-and-apis-from-unified-environment/images/2-1-3-click-deploy.png" width="800" /> |

| **2.2** | **Monitor the roll-out** |
| :--- | :--- |
| **Narration** | The team configures the pipeline to complete a rolling upgrade of the IBM MQ queue managers. As each queue manager is updated with the additional configuration of the streaming queue, mobile users will start to see notifications in their mobile application. The additional traffic will be visible in the dashboard via the notification service chart. <br/><br/>The team can see the upgrade ripple through the queue managers, as the notification service starts to receive half the traffic and then all the traffic. |
| **Action** &nbsp; 2.2.1 | Show the MQ version numbers in the application dashboard. <br/><br/> <img src="https://raw.githubusercontent.com/ibm-garage-tsa/platinum-demos/master/src/pages/300-integration-managing-events-and-apis-from-unified-environment/images/2-2-1-version-numbers.png" width="800" /> |

<br/>

**[Go to top](#place1)**

</details>


<details>

<summary>Summary</summary>

<br/>

You’ve seen how Focus Bank can release new versions and scale their cloud native integration without impacting the mobile application. This has transformed the effort required to maintain the solution. The team can respond to requests to increase the capacity or to apply fixes immediately. 

<br/>

Thank you for attending today’s presentation.

<br/>

**[Go to top](#place1)**

</details>
